{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text sentiment emotion analysis"
      ],
      "metadata": {
        "id": "QHA9FPFVvV3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Patient"
      ],
      "metadata": {
        "id": "4cgBHdPBwcz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bond: questions 3,5,7,9\n",
        "#goal: 1,4,8,11\n",
        "#task: 2,6,10,12"
      ],
      "metadata": {
        "id": "lK43rFotQo--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/emotiontext_incl_patWAI.csv', delimiter=',')\n",
        "\n",
        "# List of column names 1-12\n",
        "columns_to_check = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
        "\n",
        "# Convert the specified columns to numeric, replacing invalid values with NaN\n",
        "df[columns_to_check] = df[columns_to_check].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Check for non-float values in the specified columns\n",
        "non_float_mask = ~df[columns_to_check].applymap(lambda x: isinstance(x, float))\n",
        "\n",
        "# Find rows where any of the specified columns have non-float values\n",
        "rows_with_non_floats = non_float_mask.any(axis=1)\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "df = df.dropna(subset=columns_to_check, how='any')\n",
        "\n",
        "# df = df.drop_duplicates(subset=['ppnr', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10'])\n",
        "df = df.drop_duplicates(subset=['ppnr','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'])\n",
        "\n",
        "# If needed, reset the index of the filtered DataFrame\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "i0iW1elOXkOF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Assuming you have a DataFrame called 'df' with the desired columns\n",
        "bond_columns = ['3', '5', '7','9']\n",
        "goal_columns = ['1', '4', '8','11']\n",
        "task_columns = ['2', '6', '10','12']\n",
        "\n",
        "# Calculate the row-wise average for the specified columns\n",
        "bond_df = df[bond_columns].mean(axis=1)\n",
        "goal_df = df[goal_columns].mean(axis=1)\n",
        "task_df = df[task_columns].mean(axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "zWe2t9IrkmA6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df #39 rows"
      ],
      "metadata": {
        "id": "WHCwNutEk-P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF"
      ],
      "metadata": {
        "id": "PX4IPOYxc09o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load data from CSV\n",
        "all_df = df\n",
        "\n",
        "\n",
        "# Convert numeric columns to float\n",
        "numeric_columns = ['ppnr', 'session']\n",
        "all_df[numeric_columns] = all_df[numeric_columns].astype(float)\n",
        "\n",
        "# Group the rows by 'ppnr' and 'sessions' columns and calculate the mean for other columns\n",
        "# df = all_df.groupby(['ppnr', 'session'], as_index=False).mean()\n",
        "df = all_df\n",
        "\n",
        "# Select features and target variable\n",
        "features = df[['neutral', 'curiosity', 'sadness', 'admiration',\n",
        "       'fear', 'disgust', 'amusement', 'confusion', 'approval', 'joy', 'love',\n",
        "       'realization', 'desire', 'annoyance', 'disapproval', 'nervousness',\n",
        "       'remorse', 'excitement', 'anger', 'disappointment', 'surprise',\n",
        "       'caring', 'grief', 'embarrassment', 'gratitude', 'pride', 'optimism',\n",
        "       'relief', 'nr_positive', 'nr_negative']]\n",
        "\n",
        "\n",
        "for i in range(12):\n",
        "    nr = str(i+1) #'t' +\n",
        "\n",
        "    outcome = all_df[nr]\n",
        "    print(\"WAI question:\", nr)\n",
        "\n",
        "    # Handle missing values in features using an imputer\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "    # Train the model\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    t.fit(features_imputed, outcome)\n",
        "\n",
        "    # Calculate out-of-bag MSE for Outcome 1\n",
        "    oob_error = 1 - t.score(features_imputed, outcome)\n",
        "    print(\"Out-of-Bag MSE for Outcome 1:\", oob_error)\n",
        "\n",
        "    # Get feature importances\n",
        "    impOOB = t.feature_importances_\n",
        "\n",
        "    # Plot feature importances\n",
        "    plt.bar(range(len(impOOB)), impOOB)\n",
        "    plt.title('Unbiased Predictor Importance Estimates')\n",
        "    plt.xlabel('Predictor variable')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xticks(range(len(impOOB)), features.columns, rotation=90)\n",
        "    # plt.show()\n",
        "\n",
        "    # Check by predicting held back data\n",
        "    ncells = features.shape[0]\n",
        "    perc = int(0.8 * ncells)\n",
        "\n",
        "    nboots = 10\n",
        "    Acc = np.empty(nboots)\n",
        "    for b in range(nboots):\n",
        "        shuf = np.random.permutation(ncells)\n",
        "        incl = shuf[:perc]\n",
        "        holdback = shuf[perc:]\n",
        "\n",
        "        Mdl = t.fit(features.iloc[incl], outcome.iloc[incl])\n",
        "        label = Mdl.predict(features.iloc[holdback])\n",
        "        Acc[b] = mean_squared_error(outcome.iloc[holdback], label)\n",
        "\n",
        "    mean_mse = np.mean(Acc)\n",
        "    print(\"Mean MSE for Outcome 1:\", mean_mse)\n",
        "\n",
        "    # Feature selection\n",
        "    sorted_indices = np.argsort(impOOB)\n",
        "    bestfeat = sorted_indices[7:]\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    MdlBF = t.fit(features.iloc[:, bestfeat], outcome)\n",
        "    oob_error_bf = 1 - MdlBF.score(features.iloc[:, bestfeat], outcome)\n",
        "    print(\"Out-of-Bag MSE (Feature Selection):\", oob_error_bf)\n",
        "\n",
        "    # Print the selected features\n",
        "    selected_features = features.columns[bestfeat]\n",
        "    print(\"Selected Features:\", selected_features)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LpgZQnjNK5DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pearson correlation"
      ],
      "metadata": {
        "id": "XoGYO2GNcxEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr, linregress\n",
        "from tabulate import tabulate\n",
        "\n",
        "correlation_results = []\n",
        "\n",
        "# Calculate correlations and slopes\n",
        "for feature in features:\n",
        "    correlated_wai_questions = []\n",
        "\n",
        "    for wai_question in range(1, 13):\n",
        "        question_col = str(wai_question)\n",
        "        val = feature\n",
        "\n",
        "        # Calculate Pearson correlation coefficient and p-value\n",
        "        correlation_coefficient, p_value = pearsonr(df[val].astype(float), df[question_col].astype(float))\n",
        "\n",
        "        if p_value < 0.05:\n",
        "            # Calculate the slope of linear regression\n",
        "            slope, _, _, _, _ = linregress(df[val].astype(float), df[question_col].astype(float))\n",
        "            correlated_wai_questions.append([f'WAI Question {wai_question}', f\"{correlation_coefficient:.2f} ({slope:.2f})\", f\"{p_value:.4f}\"])\n",
        "\n",
        "    if correlated_wai_questions:\n",
        "        correlation_results.append([feature, correlated_wai_questions])\n",
        "\n",
        "# Print results in a table\n",
        "if correlation_results:\n",
        "    headers = [\"Feature\", \"WAI (Factors)\",\"Correlation Coefficient (Slope)\", \"p-value\"]\n",
        "    table_data = []\n",
        "    for result in correlation_results:\n",
        "        feature_name = result[0]\n",
        "        for corr_data in result[1]:\n",
        "            table_data.append([feature_name] + corr_data)\n",
        "\n",
        "    table = tabulate(table_data, headers=headers, tablefmt=\"pretty\")\n",
        "    # Save the table as a vector PDF\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.axis('off')\n",
        "    plt.table(cellText=table_data, colLabels=headers, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(headers))\n",
        "    plt.savefig('correlation_table_p_textfeat.pdf', format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Correlation Results for Patient scores Text Features:\")\n",
        "    print(table)\n",
        "else:\n",
        "    print(\"No correlations with p-value < 0.05 found.\")\n"
      ],
      "metadata": {
        "id": "LyeG1geMkn2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANOVA"
      ],
      "metadata": {
        "id": "lNbpN6Xwc2yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "for i in range(12):\n",
        "  nr =   str(i + 1)\n",
        "  wai_scores = df[nr].astype('category')\n",
        "  for feature in features:\n",
        "      groups = []\n",
        "      for category in wai_scores.cat.categories:\n",
        "          groups.append(df[feature][wai_scores == category])\n",
        "\n",
        "      # Perform ANOVA test\n",
        "      f_statistic, p_value = f_oneway(*groups)\n",
        "\n",
        "      if p_value < 0.05:\n",
        "          print(\"Question: \",nr)\n",
        "          print(\"Feature:\", feature)\n",
        "          print(\"F-Statistic:\", f_statistic)\n",
        "          print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "Lu8_Go9wtbaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## heatmap for correlation between features"
      ],
      "metadata": {
        "id": "acz_wSY6W6ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the features\n",
        "features = ['neutral', 'curiosity', 'sadness', 'admiration',\n",
        "       'fear', 'disgust', 'amusement', 'confusion', 'approval', 'joy', 'love',\n",
        "       'realization', 'desire', 'annoyance', 'disapproval', 'nervousness',\n",
        "       'remorse', 'excitement', 'anger', 'disappointment', 'surprise',\n",
        "       'caring', 'grief', 'embarrassment', 'gratitude', 'pride', 'optimism',\n",
        "       'relief', 'nr_positive', 'nr_negative']\n",
        "\n",
        "# Create a correlation matrix\n",
        "correlation_matrix = df[features].corr()\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XCxCP3d9W7mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Therapist"
      ],
      "metadata": {
        "id": "MJ_wkus8a8jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Therapist\n",
        "#bond: 2,5,7,9\n",
        "#goal: 3,4,8,6\n",
        "#task: 1,2,10\n",
        "\n",
        "# Patient + Observer\n",
        "#bond: 3,5,7,9\n",
        "#goal: 1,4,8,11\n",
        "#task: 2,6,10,12"
      ],
      "metadata": {
        "id": "5xorX3-Ua-Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/emotiontext_incl_tWAI.csv', delimiter=',')\n",
        "\n",
        "# List of column names\n",
        "columns_to_check = ['t1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10']\n",
        "\n",
        "# Convert the specified columns to numeric, replacing invalid values with NaN\n",
        "df[columns_to_check] = df[columns_to_check].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Check for non-float values in the specified columns\n",
        "non_float_mask = ~df[columns_to_check].applymap(lambda x: isinstance(x, float))\n",
        "\n",
        "# Find rows where any of the specified columns have non-float values\n",
        "rows_with_non_floats = non_float_mask.any(axis=1)\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "df = df.dropna(subset=columns_to_check, how='any')\n",
        "\n",
        "df = df.drop_duplicates(subset=['ppnr', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10'])\n",
        "\n",
        "# If needed, reset the index of the filtered DataFrame\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "krVhn5sMbVxk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Assuming you have a DataFrame called 'df' with the desired columns\n",
        "bond_columns = ['t2', 't5', 't7','t9']\n",
        "goal_columns = ['t3', 't4', 't8']\n",
        "task_columns = ['t1', 't2', 't6', 't10']\n",
        "\n",
        "# Calculate the row-wise average for the specified columns\n",
        "bond_df = df[bond_columns].mean(axis=1)\n",
        "goal_df = df[goal_columns].mean(axis=1)\n",
        "task_df = df[task_columns].mean(axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "9WlfMHxlbVxl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df #48 rows"
      ],
      "metadata": {
        "id": "hjmjWlu556C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF"
      ],
      "metadata": {
        "id": "rxVpYBSac4nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load data from CSV\n",
        "all_df = df\n",
        "\n",
        "# Convert numeric columns to float\n",
        "numeric_columns = ['ppnr', 'session']\n",
        "all_df[numeric_columns] = all_df[numeric_columns].astype(float)\n",
        "\n",
        "df = all_df\n",
        "\n",
        "# Select features and target variable\n",
        "features = df[['neutral', 'curiosity', 'sadness', 'admiration',\n",
        "       'fear', 'disgust', 'amusement', 'confusion', 'approval', 'joy', 'love',\n",
        "       'realization', 'desire', 'annoyance', 'disapproval', 'nervousness',\n",
        "       'remorse', 'excitement', 'anger', 'disappointment', 'surprise',\n",
        "       'caring', 'grief', 'embarrassment', 'gratitude', 'pride', 'optimism',\n",
        "       'relief', 'nr_positive', 'nr_negative']]\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    nr = 't' + str(i+1) #'t' +\n",
        "\n",
        "    outcome = all_df[nr]\n",
        "    print(\"WAI question:\", nr)\n",
        "\n",
        "    # Handle missing values in features using an imputer\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "    # Train the model\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    t.fit(features_imputed, outcome)\n",
        "\n",
        "    # Calculate out-of-bag MSE for Outcome 1\n",
        "    oob_error = 1 - t.score(features_imputed, outcome)\n",
        "    print(\"Out-of-Bag MSE for Outcome 1:\", oob_error)\n",
        "\n",
        "    # Get feature importances\n",
        "    impOOB = t.feature_importances_\n",
        "\n",
        "    # Plot feature importances\n",
        "    plt.bar(range(len(impOOB)), impOOB)\n",
        "    plt.title('Unbiased Predictor Importance Estimates')\n",
        "    plt.xlabel('Predictor variable')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xticks(range(len(impOOB)), features.columns, rotation=90)\n",
        "    # plt.show()\n",
        "\n",
        "    # Check by predicting held back data\n",
        "    ncells = features.shape[0]\n",
        "    perc = int(0.8 * ncells)\n",
        "\n",
        "    nboots = 10\n",
        "    Acc = np.empty(nboots)\n",
        "    for b in range(nboots):\n",
        "        shuf = np.random.permutation(ncells)\n",
        "        incl = shuf[:perc]\n",
        "        holdback = shuf[perc:]\n",
        "\n",
        "        Mdl = t.fit(features.iloc[incl], outcome.iloc[incl])\n",
        "        label = Mdl.predict(features.iloc[holdback])\n",
        "        Acc[b] = mean_squared_error(outcome.iloc[holdback], label)\n",
        "\n",
        "    mean_mse = np.mean(Acc)\n",
        "    print(\"Mean MSE for Outcome 1:\", mean_mse)\n",
        "\n",
        "    # Feature selection\n",
        "    sorted_indices = np.argsort(impOOB)\n",
        "    bestfeat = sorted_indices[7:]\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    MdlBF = t.fit(features.iloc[:, bestfeat], outcome)\n",
        "    oob_error_bf = 1 - MdlBF.score(features.iloc[:, bestfeat], outcome)\n",
        "    print(\"Out-of-Bag MSE (Feature Selection):\", oob_error_bf)\n",
        "\n",
        "    # Print the selected features\n",
        "    selected_features = features.columns[bestfeat]\n",
        "    print(\"Selected Features:\", selected_features)"
      ],
      "metadata": {
        "id": "FcWq6xPTbVxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pearson Correlation"
      ],
      "metadata": {
        "id": "bJvnRXJpc6ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr, linregress\n",
        "from tabulate import tabulate\n",
        "\n",
        "correlation_results = []\n",
        "\n",
        "# Calculate correlations and slopes\n",
        "for feature in features:\n",
        "    correlated_wai_questions = []\n",
        "\n",
        "    for wai_question in range(1, 11):\n",
        "        question_col = 't' + str(wai_question)\n",
        "        val = feature\n",
        "\n",
        "        # Calculate Pearson correlation coefficient and p-value\n",
        "        correlation_coefficient, p_value = pearsonr(df[val].astype(float), df[question_col].astype(float))\n",
        "\n",
        "        if p_value < 0.05:\n",
        "            # Calculate the slope of linear regression\n",
        "            slope, _, _, _, _ = linregress(df[val].astype(float), df[question_col].astype(float))\n",
        "            correlated_wai_questions.append([f'WAI Question {wai_question}', f\"{correlation_coefficient:.2f} ({slope:.2f})\", f\"{p_value:.4f}\"])\n",
        "\n",
        "    if correlated_wai_questions:\n",
        "        correlation_results.append([feature, correlated_wai_questions])\n",
        "\n",
        "# Print results in a table\n",
        "if correlation_results:\n",
        "    headers = [\"Feature\", \"WAI (Factors)\",\"Correlation Coefficient (Slope)\", \"p-value\"]\n",
        "    table_data = []\n",
        "    for result in correlation_results:\n",
        "        feature_name = result[0]\n",
        "        for corr_data in result[1]:\n",
        "            table_data.append([feature_name] + corr_data)\n",
        "\n",
        "    table = tabulate(table_data, headers=headers, tablefmt=\"pretty\")\n",
        "    # Save the table as a vector PDF\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.axis('off')\n",
        "    plt.table(cellText=table_data, colLabels=headers, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(headers))\n",
        "    plt.savefig('correlation_table_t_textfeat.pdf', format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Correlation Results for Therapist scores Text Features:\")\n",
        "    print(table)\n",
        "else:\n",
        "    print(\"No correlations with p-value < 0.05 found.\")"
      ],
      "metadata": {
        "id": "5Fe68wdybVxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANOVA"
      ],
      "metadata": {
        "id": "JBq3piI6c9Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "for i in range(10):\n",
        "  nr =  't' + str(i + 1)\n",
        "  wai_scores = df[nr].astype('category')\n",
        "  for feature in features:\n",
        "      groups = []\n",
        "      for category in wai_scores.cat.categories:\n",
        "          groups.append(df[feature][wai_scores == category])\n",
        "\n",
        "      # Perform ANOVA test\n",
        "      f_statistic, p_value = f_oneway(*groups)\n",
        "\n",
        "      if p_value < 0.05:\n",
        "          print(\"Question: \",nr)\n",
        "          print(\"Feature:\", feature)\n",
        "          print(\"F-Statistic:\", f_statistic)\n",
        "          print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "blTih_UXbVxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## heatmap for correlation between features"
      ],
      "metadata": {
        "id": "fL_B8e_GbVxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the features\n",
        "features = ['valence', 'arousal', 'dominance']\n",
        "\n",
        "# Create a correlation matrix\n",
        "correlation_matrix = df[features].corr()\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "68BKa2sQbVxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observer scores"
      ],
      "metadata": {
        "id": "nslyXpwksj5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/emotiontext_incl_oWAI.csv', delimiter=',')\n",
        "\n",
        "new_column_names = {str(i): f'o{i}' for i in range(1, 13)}\n",
        "df.rename(columns=new_column_names, inplace=True)\n",
        "\n",
        "# List of column names\n",
        "columns_to_check = ['o1', 'o2', 'o3', 'o4', 'o5', 'o6', 'o7', 'o8', 'o9', 'o10','o11', 'o12']\n",
        "\n",
        "# Convert the specified columns to numeric, replacing invalid values with NaN\n",
        "df[columns_to_check] = df[columns_to_check].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Check for non-float values in the specified columns\n",
        "non_float_mask = ~df[columns_to_check].applymap(lambda x: isinstance(x, float))\n",
        "\n",
        "# Find rows where any of the specified columns have non-float values\n",
        "rows_with_non_floats = non_float_mask.any(axis=1)\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "df = df.dropna(subset=columns_to_check, how='any')\n",
        "\n",
        "df = df.drop_duplicates(subset=['ppnr', 'o1', 'o2', 'o3', 'o4', 'o5', 'o6', 'o7', 'o8', 'o9', 'o10','o11', 'o12'])\n",
        "\n",
        "# If needed, reset the index of the filtered DataFrame\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "2ZGGSVm2smh5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df #75 rows"
      ],
      "metadata": {
        "id": "-v80jI-KzqXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Assuming you have a DataFrame called 'df' with the desired columns\n",
        "bond_columns = ['o3', 'o5', 'o7','o9']\n",
        "goal_columns = ['o1', 'o4', 'o8','o11']\n",
        "task_columns = ['o2', 'o6', 'o10','o12']\n",
        "\n",
        "# Calculate the row-wise average for the specified columns\n",
        "bond_df = df[bond_columns].mean(axis=1)\n",
        "goal_df = df[goal_columns].mean(axis=1)\n",
        "task_df = df[task_columns].mean(axis=1)\n",
        "\n",
        "# Calculate the row-wise average for the specified columns\n",
        "bond_df = df[bond_columns].mean(axis=1)\n",
        "goal_df = df[goal_columns].mean(axis=1)\n",
        "task_df = df[task_columns].mean(axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "4EpWXgDysmiB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF"
      ],
      "metadata": {
        "id": "E-xQ_C1dsmiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load data from CSV\n",
        "all_df = df\n",
        "\n",
        "# Convert numeric columns to float\n",
        "numeric_columns = ['ppnr', 'session']\n",
        "all_df[numeric_columns] = all_df[numeric_columns].astype(float)\n",
        "\n",
        "df = all_df\n",
        "\n",
        "# Select features and target variable\n",
        "features = df[['neutral', 'curiosity', 'sadness', 'admiration',\n",
        "       'fear', 'disgust', 'amusement', 'confusion', 'approval', 'joy', 'love',\n",
        "       'realization', 'desire', 'annoyance', 'disapproval', 'nervousness',\n",
        "       'remorse', 'excitement', 'anger', 'disappointment', 'surprise',\n",
        "       'caring', 'grief', 'embarrassment', 'gratitude', 'pride', 'optimism',\n",
        "       'relief', 'nr_positive', 'nr_negative']]\n",
        "\n",
        "\n",
        "for i in range(12):\n",
        "    nr = 'o' + str(i+1)\n",
        "\n",
        "    outcome = all_df[nr]\n",
        "    print(\"WAI question:\", nr)\n",
        "\n",
        "    # Handle missing values in features using an imputer\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "    # Train the model\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    t.fit(features_imputed, outcome)\n",
        "\n",
        "    # Calculate out-of-bag MSE for Outcome 1\n",
        "    oob_error = 1 - t.score(features_imputed, outcome)\n",
        "    print(\"Out-of-Bag MSE for Outcome 1:\", oob_error)\n",
        "\n",
        "    # Get feature importances\n",
        "    impOOB = t.feature_importances_\n",
        "\n",
        "    # Plot feature importances\n",
        "    plt.bar(range(len(impOOB)), impOOB)\n",
        "    plt.title('Unbiased Predictor Importance Estimates')\n",
        "    plt.xlabel('Predictor variable')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xticks(range(len(impOOB)), features.columns, rotation=90)\n",
        "    # plt.show()\n",
        "\n",
        "    # Check by predicting held back data\n",
        "    ncells = features.shape[0]\n",
        "    perc = int(0.8 * ncells)\n",
        "\n",
        "    nboots = 10\n",
        "    Acc = np.empty(nboots)\n",
        "    for b in range(nboots):\n",
        "        shuf = np.random.permutation(ncells)\n",
        "        incl = shuf[:perc]\n",
        "        holdback = shuf[perc:]\n",
        "\n",
        "        Mdl = t.fit(features.iloc[incl], outcome.iloc[incl])\n",
        "        label = Mdl.predict(features.iloc[holdback])\n",
        "        Acc[b] = mean_squared_error(outcome.iloc[holdback], label)\n",
        "\n",
        "    mean_mse = np.mean(Acc)\n",
        "    print(\"Mean MSE for Outcome 1:\", mean_mse)\n",
        "\n",
        "\n",
        "    # Feature selection\n",
        "    sorted_indices = np.argsort(impOOB)\n",
        "    bestfeat = sorted_indices[7:]\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    MdlBF = t.fit(features.iloc[:, bestfeat], outcome)\n",
        "    oob_error_bf = 1 - MdlBF.score(features.iloc[:, bestfeat], outcome)\n",
        "    print(\"Out-of-Bag MSE (Feature Selection):\", oob_error_bf)\n",
        "\n",
        "    # Print the selected features\n",
        "    selected_features = features.columns[bestfeat]\n",
        "    print(\"Selected Features:\", selected_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ubss1g3gsmiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pearson Correlation"
      ],
      "metadata": {
        "id": "r2HLVmYjsmiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr, linregress\n",
        "from tabulate import tabulate\n",
        "\n",
        "correlation_results = []\n",
        "\n",
        "# Calculate correlations and slopes\n",
        "for feature in features:\n",
        "    correlated_wai_questions = []\n",
        "\n",
        "    for wai_question in range(1, 13):\n",
        "        question_col = 'o' + str(wai_question)\n",
        "        val = feature\n",
        "\n",
        "        # Calculate Pearson correlation coefficient and p-value\n",
        "        correlation_coefficient, p_value = pearsonr(df[val].astype(float), df[question_col].astype(float))\n",
        "\n",
        "        if p_value < 0.05:\n",
        "            # Calculate the slope of linear regression\n",
        "            slope, _, _, _, _ = linregress(df[val].astype(float), df[question_col].astype(float))\n",
        "            correlated_wai_questions.append([f'WAI Question {wai_question}', f\"{correlation_coefficient:.2f} ({slope:.2f})\", f\"{p_value:.4f}\"])\n",
        "\n",
        "    if correlated_wai_questions:\n",
        "        correlation_results.append([feature, correlated_wai_questions])\n",
        "\n",
        "# Print results in a table\n",
        "if correlation_results:\n",
        "    headers = [\"Feature\", \"WAI (Factors)\",\"Correlation Coefficient (Slope)\", \"p-value\"]\n",
        "    table_data = []\n",
        "    for result in correlation_results:\n",
        "        feature_name = result[0]\n",
        "        for corr_data in result[1]:\n",
        "            table_data.append([feature_name] + corr_data)\n",
        "\n",
        "    table = tabulate(table_data, headers=headers, tablefmt=\"pretty\")\n",
        "    # Save the table as a vector PDF\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.axis('off')\n",
        "    plt.table(cellText=table_data, colLabels=headers, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(headers))\n",
        "    plt.savefig('correlation_table_o_textfeat.pdf', format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Correlation Results for the observer scores Text Features:\")\n",
        "    print(table)\n",
        "else:\n",
        "    print(\"No correlations with p-value < 0.05 found.\")"
      ],
      "metadata": {
        "id": "aviYSrTxsmiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "for i in range(12):\n",
        "  nr =  'o' + str(i + 1)\n",
        "  wai_scores = df[nr].astype('category')\n",
        "  for feature in features:\n",
        "      groups = []\n",
        "      for category in wai_scores.cat.categories:\n",
        "          groups.append(df[feature][wai_scores == category])\n",
        "\n",
        "      # Perform ANOVA test\n",
        "      f_statistic, p_value = f_oneway(*groups)\n",
        "\n",
        "      if p_value < 0.05:\n",
        "          print(\"Question: \",nr)\n",
        "          print(\"Feature:\", feature)\n",
        "          print(\"F-Statistic:\", f_statistic)\n",
        "          print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "DAuvm_uv2ft9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## heatmap for correlation between features"
      ],
      "metadata": {
        "id": "EplIa49m2fuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the features\n",
        "features = ['valence', 'arousal', 'dominance']\n",
        "\n",
        "# Create a correlation matrix\n",
        "correlation_matrix = df[features].corr()\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GBk3PDe12fuN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
