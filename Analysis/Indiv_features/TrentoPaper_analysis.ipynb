{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Patient scores"
      ],
      "metadata": {
        "id": "gxoYEOEDQ84n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bond: questions 3,5,7,9\n",
        "#goal: 1,4,8,11\n",
        "#task: 2,6,10,12"
      ],
      "metadata": {
        "id": "lK43rFotQo--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/Users/rivka/Desktop/Final/Trentofeat_incl_patWAI2.csv', delimiter=',')\n",
        "\n",
        "# List of column names 1-12\n",
        "columns_to_check = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
        "columns_to_check_t = ['t1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10']\n",
        "\n",
        "# Convert the specified columns to numeric, replacing invalid values with NaN\n",
        "df[columns_to_check] = df[columns_to_check].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Check for non-float values in the specified columns\n",
        "non_float_mask = ~df[columns_to_check].applymap(lambda x: isinstance(x, float))\n",
        "\n",
        "# Find rows where any of the specified columns have non-float values\n",
        "rows_with_non_floats = non_float_mask.any(axis=1)\n",
        "\n",
        "# Remove rows with non-float values\n",
        "df = df.drop_duplicates()\n",
        "df = df.dropna(subset=columns_to_check, how='any')\n",
        "\n",
        "# df = df.drop_duplicates(subset=['ppnr', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10'])\n",
        "df = df.drop_duplicates(subset=['ppnr','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'])\n",
        "\n",
        "# If needed, reset the index of the filtered DataFrame\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "i0iW1elOXkOF"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame called 'df' with the desired columns\n",
        "bond_columns = ['3', '5', '7','9']\n",
        "goal_columns = ['1', '4', '8','11']\n",
        "task_columns = ['2', '6', '10','12']\n",
        "\n",
        "# Calculate the row-wise average for the specified columns\n",
        "bond_df = df[bond_columns].mean(axis=1)\n",
        "goal_df = df[goal_columns].mean(axis=1)\n",
        "task_df = df[task_columns].mean(axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "zWe2t9IrkmA6"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df #45 rows"
      ],
      "metadata": {
        "id": "WHCwNutEk-P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF"
      ],
      "metadata": {
        "id": "PX4IPOYxc09o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load data from CSV\n",
        "all_df = df\n",
        "\n",
        "# Convert numeric columns to float\n",
        "numeric_columns = ['ppnr', 'session']\n",
        "all_df[numeric_columns] = all_df[numeric_columns].astype(float)\n",
        "\n",
        "# Group the rows by 'ppnr' and 'sessions' columns and calculate the mean for other columns\n",
        "# df = all_df.groupby(['ppnr', 'session'], as_index=False).mean()\n",
        "df = all_df\n",
        "\n",
        "# Select features and target variable\n",
        "features = df[[ 'mean_waittime', 'median_waittime', 'std_waittime','nr_of_turns', 'turns_therapist',\n",
        "       'turns_speaker_patient', 'duration_therapist', 'duration_patient',\n",
        "       'participation_equality', 'turn_level_freedom',\n",
        "       'avgturnlength_therapist', 'avgturnlength_patient',\n",
        "       'wordcount_speaker_therapist', 'wordcount_speaker_patient',\n",
        "       'speechrate_speaker_therapist', 'speechrate_speaker_patient',\n",
        "       'speechduration_therapist', 'speechduration_patient',\n",
        "       'durationpercentage_therapist', 'durationpercentage_patient']]\n",
        "\n",
        "\n",
        "for i in range(12):\n",
        "    nr = str(i+1) #'t' +\n",
        "\n",
        "    outcome = all_df[nr]\n",
        "    print(\"WAI question:\", nr)\n",
        "\n",
        "    # Handle missing values in features using an imputer\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "    # Train the model\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    t.fit(features_imputed, outcome)\n",
        "\n",
        "    # Calculate out-of-bag MSE for Outcome 1\n",
        "    oob_error = 1 - t.score(features_imputed, outcome)\n",
        "    print(\"Out-of-Bag MSE for Outcome 1:\", oob_error)\n",
        "\n",
        "    # Get feature importances\n",
        "    impOOB = t.feature_importances_\n",
        "\n",
        "    # Plot feature importances\n",
        "    plt.bar(range(len(impOOB)), impOOB)\n",
        "    plt.title('Unbiased Predictor Importance Estimates')\n",
        "    plt.xlabel('Predictor variable')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xticks(range(len(impOOB)), features.columns, rotation=90)\n",
        "    # plt.show()\n",
        "\n",
        "    # Check by predicting held back data\n",
        "    ncells = features.shape[0]\n",
        "    perc = int(0.8 * ncells)\n",
        "\n",
        "    nboots = 10\n",
        "    Acc = np.empty(nboots)\n",
        "    for b in range(nboots):\n",
        "        shuf = np.random.permutation(ncells)\n",
        "        incl = shuf[:perc]\n",
        "        holdback = shuf[perc:]\n",
        "\n",
        "        Mdl = t.fit(features.iloc[incl], outcome.iloc[incl])\n",
        "        label = Mdl.predict(features.iloc[holdback])\n",
        "        Acc[b] = mean_squared_error(outcome.iloc[holdback], label)\n",
        "\n",
        "    mean_mse = np.mean(Acc)\n",
        "    print(\"Mean MSE for Outcome 1:\", mean_mse)\n",
        "\n",
        "\n",
        "    # Feature selection\n",
        "    sorted_indices = np.argsort(impOOB)\n",
        "    bestfeat = sorted_indices[7:]\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    MdlBF = t.fit(features.iloc[:, bestfeat], outcome)\n",
        "    oob_error_bf = 1 - MdlBF.score(features.iloc[:, bestfeat], outcome)\n",
        "    print(\"Out-of-Bag MSE (Feature Selection):\", oob_error_bf)\n",
        "\n",
        "    # Print the selected features\n",
        "    selected_features = features.columns[bestfeat]\n",
        "    print(\"Selected Features:\", selected_features)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LpgZQnjNK5DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF for components"
      ],
      "metadata": {
        "id": "rPsLufr5KlrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "df = all_df\n",
        "\n",
        "# Select features\n",
        "features = df[['nr_of_turns', 'turns_therapist',\n",
        "       'turns_speaker_patient', 'duration_therapist', 'duration_patient',\n",
        "       'participation_equality', 'turn_level_freedom',\n",
        "       'avgturnlength_therapist', 'avgturnlength_patient',\n",
        "       'wordcount_speaker_therapist', 'wordcount_speaker_patient',\n",
        "       'speechrate_speaker_therapist', 'speechrate_speaker_patient',\n",
        "       'speechduration_therapist', 'speechduration_patient',\n",
        "       'durationpercentage_therapist', 'durationpercentage_patient']]\n",
        "\n",
        "# Prepare outcome variables\n",
        "outcome_bond = bond_df\n",
        "outcome_goal = goal_df\n",
        "outcome_task = task_df\n",
        "\n",
        "# Handle missing values in features using an imputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "# Train the model for bond_df\n",
        "t_bond = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "t_bond.fit(features_imputed, outcome_bond)\n",
        "\n",
        "# Train the model for goal_df\n",
        "t_goal = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "t_goal.fit(features_imputed, outcome_goal)\n",
        "\n",
        "# Train the model for task_df\n",
        "t_task = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "t_task.fit(features_imputed, outcome_task)\n",
        "\n",
        "# Calculate out-of-bag MSE for bond_df\n",
        "oob_error_bond = 1 - t_bond.score(features_imputed, outcome_bond)\n",
        "print(\"Out-of-Bag MSE for bond_df:\", oob_error_bond)\n",
        "\n",
        "# Calculate out-of-bag MSE for goal_df\n",
        "oob_error_goal = 1 - t_goal.score(features_imputed, outcome_goal)\n",
        "print(\"Out-of-Bag MSE for goal_df:\", oob_error_goal)\n",
        "\n",
        "# Calculate out-of-bag MSE for task_df\n",
        "oob_error_task = 1 - t_task.score(features_imputed, outcome_task)\n",
        "print(\"Out-of-Bag MSE for task_df:\", oob_error_task)\n",
        "\n",
        "# Feature importances for bond_df\n",
        "print(\"Feature importances for bond_df:\", t_bond.feature_importances_)\n",
        "\n",
        "# Feature importances for goal_df\n",
        "print(\"Feature importances for goal_df:\", t_goal.feature_importances_)\n",
        "\n",
        "# Feature importances for task_df\n",
        "print(\"Feature importances for task_df:\", t_task.feature_importances_)\n",
        "\n",
        "# Plots (if desired)\n",
        "# Plot feature importances for bond_df\n",
        "impOOB_bond = t_bond.feature_importances_\n",
        "plt.bar(range(len(impOOB_bond)), impOOB_bond)\n",
        "plt.title('Feature Importance for bond_df')\n",
        "plt.xlabel('Predictor variable')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(range(len(impOOB_bond)), features.columns, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances for goal_df\n",
        "impOOB_goal = t_goal.feature_importances_\n",
        "plt.bar(range(len(impOOB_goal)), impOOB_goal)\n",
        "plt.title('Feature Importance for goal_df')\n",
        "plt.xlabel('Predictor variable')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(range(len(impOOB_goal)), features.columns, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances for task_df\n",
        "impOOB_task = t_task.feature_importances_\n",
        "plt.bar(range(len(impOOB_task)), impOOB_task)\n",
        "plt.title('Feature Importance for task_df')\n",
        "plt.xlabel('Predictor variable')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(range(len(impOOB_task)), features.columns, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Cross-validation for bond_df\n",
        "predicted_bond = cross_val_predict(t_bond, features_imputed, outcome_bond, cv=10)\n",
        "mse_bond = mean_squared_error(outcome_bond, predicted_bond)\n",
        "print(\"Cross-validation MSE for bond_df:\", mse_bond)\n",
        "\n",
        "# Cross-validation for goal_df\n",
        "predicted_goal = cross_val_predict(t_goal, features_imputed, outcome_goal, cv=10)\n",
        "mse_goal = mean_squared_error(outcome_goal, predicted_goal)\n",
        "print(\"Cross-validation MSE for goal_df:\", mse_goal)\n",
        "\n",
        "# Cross-validation for task_df\n",
        "predicted_task = cross_val_predict(t_task, features_imputed, outcome_task, cv=10)\n",
        "mse_task = mean_squared_error(outcome_task, predicted_task)\n",
        "print(\"Cross-validation MSE for task_df:\", mse_task)\n",
        "\n",
        "# Mean MSE for bond_df\n",
        "mean_mse_bond = np.mean(mse_bond)\n",
        "print(\"Mean MSE for bond_df:\", mean_mse_bond)\n",
        "\n",
        "# Mean MSE for goal_df\n",
        "mean_mse_goal = np.mean(mse_goal)\n",
        "print(\"Mean MSE for goal_df:\", mean_mse_goal)\n",
        "\n",
        "# Mean MSE for task_df\n",
        "mean_mse_task = np.mean(mse_task)\n",
        "print(\"Mean MSE for task_df:\", mean_mse_task)\n"
      ],
      "metadata": {
        "id": "lEKCOj_qKnST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pearson correlation"
      ],
      "metadata": {
        "id": "XoGYO2GNcxEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr, linregress\n",
        "from tabulate import tabulate\n",
        "\n",
        "correlation_results = []\n",
        "\n",
        "# Calculate correlations and slopes\n",
        "for feature in features:\n",
        "    correlated_wai_questions = []\n",
        "\n",
        "    for wai_question in range(1, 13):\n",
        "        question_col = str(wai_question)\n",
        "        val = feature\n",
        "\n",
        "        # Calculate Pearson correlation coefficient and p-value\n",
        "        correlation_coefficient, p_value = pearsonr(df[val].astype(float), df[question_col].astype(float))\n",
        "\n",
        "        if p_value < 0.05:\n",
        "            # Calculate the slope of linear regression\n",
        "            slope, _, _, _, _ = linregress(df[val].astype(float), df[question_col].astype(float))\n",
        "            correlated_wai_questions.append([f'WAI Question {wai_question}', f\"{correlation_coefficient:.2f} ({slope:.2f})\", f\"{p_value:.4f}\"])\n",
        "\n",
        "    if correlated_wai_questions:\n",
        "        correlation_results.append([feature, correlated_wai_questions])\n",
        "\n",
        "# Print results in a table\n",
        "if correlation_results:\n",
        "    headers = [\"Feature\", \"WAI (Factors)\",\"Correlation Coefficient (Slope)\", \"p-value\"]\n",
        "    table_data = []\n",
        "    for result in correlation_results:\n",
        "        feature_name = result[0]\n",
        "        for corr_data in result[1]:\n",
        "            table_data.append([feature_name] + corr_data)\n",
        "\n",
        "    table = tabulate(table_data, headers=headers, tablefmt=\"pretty\")\n",
        "    # Save the table as a vector PDF\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.axis('off')\n",
        "    plt.table(cellText=table_data, colLabels=headers, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(headers))\n",
        "    plt.savefig('correlation_table_p.pdf', format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Correlation Results for Patient scores Trento features:\")\n",
        "    print(table)\n",
        "else:\n",
        "    print(\"No correlations with p-value < 0.05 found.\")\n"
      ],
      "metadata": {
        "id": "LyeG1geMkn2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANOVA"
      ],
      "metadata": {
        "id": "lNbpN6Xwc2yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "from tabulate import tabulate\n",
        "\n",
        "anova_results = []\n",
        "\n",
        "# Perform ANOVA test for each question and feature\n",
        "for i in range(12):\n",
        "    nr = str(i + 1)\n",
        "    wai_scores = df[nr].astype('category')\n",
        "    for feature in features:\n",
        "        groups = []\n",
        "        for category in wai_scores.cat.categories:\n",
        "            groups.append(df[feature][wai_scores == category])\n",
        "\n",
        "        # Perform ANOVA test\n",
        "        f_statistic, p_value = f_oneway(*groups)\n",
        "\n",
        "        if p_value < 0.05:\n",
        "            anova_results.append([f'WAI Question {nr}', feature, f\"{f_statistic:.2f}\", f\"{p_value:.4f}\"])\n",
        "\n",
        "# Print results in a table\n",
        "if anova_results:\n",
        "    headers = [\"WAI Question\", \"Feature\", \"F-Statistic\", \"p-value\"]\n",
        "    table = tabulate(anova_results, headers=headers, tablefmt=\"pretty\")\n",
        "\n",
        "    # Save the table as a vector PDF\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.axis('off')\n",
        "    plt.table(cellText=anova_results, colLabels=headers, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(headers))\n",
        "    plt.savefig('anova_table_p.pdf', format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"ANOVA Results:\")\n",
        "    print(table)\n",
        "else:\n",
        "    print(\"No significant ANOVA results (p-value < 0.05) found.\")\n"
      ],
      "metadata": {
        "id": "Lu8_Go9wtbaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## heatmap for correlation between features"
      ],
      "metadata": {
        "id": "acz_wSY6W6ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the features\n",
        "features = ['nr_of_turns', 'turns_therapist', 'turns_speaker_patient', 'duration_therapist', 'duration_patient',\n",
        "            'participation_equality', 'turn_level_freedom', 'avgturnlength_therapist', 'avgturnlength_patient',\n",
        "            'wordcount_speaker_therapist', 'wordcount_speaker_patient', 'speechrate_speaker_therapist',\n",
        "            'speechrate_speaker_patient', 'speechduration_therapist', 'speechduration_patient',\n",
        "            'durationpercentage_therapist', 'durationpercentage_patient']\n",
        "\n",
        "# Create a correlation matrix\n",
        "correlation_matrix = df[features].corr()\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title(\"heatmap correlations between features for patient WAI ratings\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XCxCP3d9W7mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Therapist"
      ],
      "metadata": {
        "id": "MJ_wkus8a8jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Therapist\n",
        "#bond: 2,5,7,9\n",
        "#goal: 3,4,8\n",
        "#task: 1,2,10\n",
        "\n",
        "Patient + Observer\n",
        "#bond: 3,5,7,9\n",
        "#goal: 1,4,8,11\n",
        "#task: 2,6,10,12"
      ],
      "metadata": {
        "id": "5xorX3-Ua-Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/Users/rivka/Desktop/Final/Trentofeat_incl_tWAI2.csv', delimiter=',')\n",
        "\n",
        "# List of column names\n",
        "columns_to_check = ['t1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10']\n",
        "\n",
        "# Convert the specified columns to numeric, replacing invalid values with NaN\n",
        "df[columns_to_check] = df[columns_to_check].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Check for non-float values in the specified columns\n",
        "non_float_mask = ~df[columns_to_check].applymap(lambda x: isinstance(x, float))\n",
        "\n",
        "# Find rows where any of the specified columns have non-float values\n",
        "rows_with_non_floats = non_float_mask.any(axis=1)\n",
        "\n",
        "# Remove rows with non-float values\n",
        "df = df.drop_duplicates()\n",
        "df = df.dropna(subset=columns_to_check, how='any')\n",
        "\n",
        "df = df.drop_duplicates(subset=['ppnr', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10'])\n",
        "\n",
        "# If needed, reset the index of the filtered DataFrame\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "krVhn5sMbVxk"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Assuming you have a DataFrame called 'df' with the desired columns\n",
        "bond_columns = ['t2', 't5', 't7','t9']\n",
        "goal_columns = ['t3', 't4', 't8']\n",
        "task_columns = ['t1', 't2', 't6', 't10']\n",
        "\n",
        "# Calculate the row-wise average for the specified columns\n",
        "bond_df = df[bond_columns].mean(axis=1)\n",
        "goal_df = df[goal_columns].mean(axis=1)\n",
        "task_df = df[task_columns].mean(axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "9WlfMHxlbVxl"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df #40 rows"
      ],
      "metadata": {
        "id": "naLx7_Mr4_Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF"
      ],
      "metadata": {
        "id": "rxVpYBSac4nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load data from CSV\n",
        "all_df = df\n",
        "\n",
        "# Convert numeric columns to float\n",
        "numeric_columns = ['ppnr', 'session']\n",
        "all_df[numeric_columns] = all_df[numeric_columns].astype(float)\n",
        "\n",
        "df = all_df\n",
        "\n",
        "# Select features and target variable\n",
        "features = df[[ 'mean_waittime', 'median_waittime', 'std_waittime','nr_of_turns', 'turns_therapist',\n",
        "       'turns_speaker_patient', 'duration_therapist', 'duration_patient',\n",
        "       'participation_equality', 'turn_level_freedom',\n",
        "       'avgturnlength_therapist', 'avgturnlength_patient',\n",
        "       'wordcount_speaker_therapist', 'wordcount_speaker_patient',\n",
        "       'speechrate_speaker_therapist', 'speechrate_speaker_patient',\n",
        "       'speechduration_therapist', 'speechduration_patient',\n",
        "       'durationpercentage_therapist', 'durationpercentage_patient']]\n",
        "\n",
        "# Prepare outcome variable\n",
        "# outcome = df['WAI_score']\n",
        "\n",
        "for i in range(10):\n",
        "    nr = 't' + str(i+1) #'t' +\n",
        "\n",
        "    outcome = all_df[nr]\n",
        "    print(\"WAI question:\", nr)\n",
        "\n",
        "    # Handle missing values in features using an imputer\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "    # Train the model\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    t.fit(features_imputed, outcome)\n",
        "\n",
        "    # Calculate out-of-bag MSE for Outcome 1\n",
        "    oob_error = 1 - t.score(features_imputed, outcome)\n",
        "    print(\"Out-of-Bag MSE for Outcome 1:\", oob_error)\n",
        "\n",
        "    # Get feature importances\n",
        "    impOOB = t.feature_importances_\n",
        "\n",
        "    # Plot feature importances\n",
        "    plt.bar(range(len(impOOB)), impOOB)\n",
        "    plt.title('Unbiased Predictor Importance Estimates')\n",
        "    plt.xlabel('Predictor variable')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xticks(range(len(impOOB)), features.columns, rotation=90)\n",
        "    # plt.show()\n",
        "\n",
        "    # Check by predicting held back data\n",
        "    ncells = features.shape[0]\n",
        "    perc = int(0.8 * ncells)\n",
        "\n",
        "    nboots = 10\n",
        "    Acc = np.empty(nboots)\n",
        "    for b in range(nboots):\n",
        "        shuf = np.random.permutation(ncells)\n",
        "        incl = shuf[:perc]\n",
        "        holdback = shuf[perc:]\n",
        "\n",
        "        Mdl = t.fit(features.iloc[incl], outcome.iloc[incl])\n",
        "        label = Mdl.predict(features.iloc[holdback])\n",
        "        Acc[b] = mean_squared_error(outcome.iloc[holdback], label)\n",
        "\n",
        "    mean_mse = np.mean(Acc)\n",
        "    print(\"Mean MSE for Outcome 1:\", mean_mse)\n",
        "\n",
        "    # Feature selection\n",
        "    sorted_indices = np.argsort(impOOB)\n",
        "    bestfeat = sorted_indices[7:]\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    MdlBF = t.fit(features.iloc[:, bestfeat], outcome)\n",
        "    oob_error_bf = 1 - MdlBF.score(features.iloc[:, bestfeat], outcome)\n",
        "    print(\"Out-of-Bag MSE (Feature Selection):\", oob_error_bf)\n",
        "\n",
        "    # Print the selected features\n",
        "    selected_features = features.columns[bestfeat]\n",
        "    print(\"Selected Features:\", selected_features)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FcWq6xPTbVxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF for components"
      ],
      "metadata": {
        "id": "QanjfEXLLD1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "df = all_df\n",
        "\n",
        "# Select features\n",
        "features = df[['nr_of_turns', 'turns_therapist',\n",
        "       'turns_speaker_patient', 'duration_therapist', 'duration_patient',\n",
        "       'participation_equality', 'turn_level_freedom',\n",
        "       'avgturnlength_therapist', 'avgturnlength_patient',\n",
        "       'wordcount_speaker_therapist', 'wordcount_speaker_patient',\n",
        "       'speechrate_speaker_therapist', 'speechrate_speaker_patient',\n",
        "       'speechduration_therapist', 'speechduration_patient',\n",
        "       'durationpercentage_therapist', 'durationpercentage_patient']]\n",
        "\n",
        "# Prepare outcome variables\n",
        "outcome_bond = bond_df\n",
        "outcome_goal = goal_df\n",
        "outcome_task = task_df\n",
        "\n",
        "# Handle missing values in features using an imputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "# Train the model for bond_df\n",
        "t_bond = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "t_bond.fit(features_imputed, outcome_bond)\n",
        "\n",
        "# Train the model for goal_df\n",
        "t_goal = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "t_goal.fit(features_imputed, outcome_goal)\n",
        "\n",
        "# Train the model for task_df\n",
        "t_task = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "t_task.fit(features_imputed, outcome_task)\n",
        "\n",
        "# Calculate out-of-bag MSE for bond_df\n",
        "oob_error_bond = 1 - t_bond.score(features_imputed, outcome_bond)\n",
        "print(\"Out-of-Bag MSE for bond_df:\", oob_error_bond)\n",
        "\n",
        "# Calculate out-of-bag MSE for goal_df\n",
        "oob_error_goal = 1 - t_goal.score(features_imputed, outcome_goal)\n",
        "print(\"Out-of-Bag MSE for goal_df:\", oob_error_goal)\n",
        "\n",
        "# Calculate out-of-bag MSE for task_df\n",
        "oob_error_task = 1 - t_task.score(features_imputed, outcome_task)\n",
        "print(\"Out-of-Bag MSE for task_df:\", oob_error_task)\n",
        "\n",
        "# Feature importances for bond_df\n",
        "print(\"Feature importances for bond_df:\", t_bond.feature_importances_)\n",
        "\n",
        "# Feature importances for goal_df\n",
        "print(\"Feature importances for goal_df:\", t_goal.feature_importances_)\n",
        "\n",
        "# Feature importances for task_df\n",
        "print(\"Feature importances for task_df:\", t_task.feature_importances_)\n",
        "\n",
        "# Plots (if desired)\n",
        "# Plot feature importances for bond_df\n",
        "impOOB_bond = t_bond.feature_importances_\n",
        "plt.bar(range(len(impOOB_bond)), impOOB_bond)\n",
        "plt.title('Feature Importance for bond_df')\n",
        "plt.xlabel('Predictor variable')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(range(len(impOOB_bond)), features.columns, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances for goal_df\n",
        "impOOB_goal = t_goal.feature_importances_\n",
        "plt.bar(range(len(impOOB_goal)), impOOB_goal)\n",
        "plt.title('Feature Importance for goal_df')\n",
        "plt.xlabel('Predictor variable')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(range(len(impOOB_goal)), features.columns, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances for task_df\n",
        "impOOB_task = t_task.feature_importances_\n",
        "plt.bar(range(len(impOOB_task)), impOOB_task)\n",
        "plt.title('Feature Importance for task_df')\n",
        "plt.xlabel('Predictor variable')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(range(len(impOOB_task)), features.columns, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Cross-validation for bond_df\n",
        "predicted_bond = cross_val_predict(t_bond, features_imputed, outcome_bond, cv=10)\n",
        "mse_bond = mean_squared_error(outcome_bond, predicted_bond)\n",
        "print(\"Cross-validation MSE for bond_df:\", mse_bond)\n",
        "\n",
        "# Cross-validation for goal_df\n",
        "predicted_goal = cross_val_predict(t_goal, features_imputed, outcome_goal, cv=10)\n",
        "mse_goal = mean_squared_error(outcome_goal, predicted_goal)\n",
        "print(\"Cross-validation MSE for goal_df:\", mse_goal)\n",
        "\n",
        "# Cross-validation for task_df\n",
        "predicted_task = cross_val_predict(t_task, features_imputed, outcome_task, cv=10)\n",
        "mse_task = mean_squared_error(outcome_task, predicted_task)\n",
        "print(\"Cross-validation MSE for task_df:\", mse_task)\n",
        "\n",
        "# Mean MSE for bond_df\n",
        "mean_mse_bond = np.mean(mse_bond)\n",
        "print(\"Mean MSE for bond_df:\", mean_mse_bond)\n",
        "\n",
        "# Mean MSE for goal_df\n",
        "mean_mse_goal = np.mean(mse_goal)\n",
        "print(\"Mean MSE for goal_df:\", mean_mse_goal)\n",
        "\n",
        "# Mean MSE for task_df\n",
        "mean_mse_task = np.mean(mse_task)\n",
        "print(\"Mean MSE for task_df:\", mean_mse_task)\n"
      ],
      "metadata": {
        "id": "HuRWfOeLLD1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pearson Correlation"
      ],
      "metadata": {
        "id": "bJvnRXJpc6ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr, linregress\n",
        "from tabulate import tabulate\n",
        "\n",
        "correlation_results = []\n",
        "\n",
        "# Calculate correlations and slopes\n",
        "for feature in features:\n",
        "    correlated_wai_questions = []\n",
        "\n",
        "    for wai_question in range(1, 11):\n",
        "        question_col = 't' + str(wai_question)\n",
        "        val = feature\n",
        "\n",
        "        # Calculate Pearson correlation coefficient and p-value\n",
        "        correlation_coefficient, p_value = pearsonr(df[val].astype(float), df[question_col].astype(float))\n",
        "\n",
        "        if p_value < 0.05:\n",
        "            # Calculate the slope of linear regression\n",
        "            slope, _, _, _, _ = linregress(df[val].astype(float), df[question_col].astype(float))\n",
        "            correlated_wai_questions.append([f'WAI Question {wai_question}', f\"{correlation_coefficient:.2f} ({slope:.2f})\", f\"{p_value:.4f}\"])\n",
        "\n",
        "    if correlated_wai_questions:\n",
        "        correlation_results.append([feature, correlated_wai_questions])\n",
        "\n",
        "# Print results in a table\n",
        "if correlation_results:\n",
        "    headers = [\"Feature\", \"WAI (Factors)\",\"Correlation Coefficient (Slope)\", \"p-value\"]\n",
        "    table_data = []\n",
        "    for result in correlation_results:\n",
        "        feature_name = result[0]\n",
        "        for corr_data in result[1]:\n",
        "            table_data.append([feature_name] + corr_data)\n",
        "\n",
        "    table = tabulate(table_data, headers=headers, tablefmt=\"pretty\")\n",
        "    # Save the table as a vector PDF\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.axis('off')\n",
        "    plt.table(cellText=table_data, colLabels=headers, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(headers))\n",
        "    plt.savefig('correlation_table_t.pdf', format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Correlation Results for Therapist scores Trento features:\")\n",
        "    print(table)\n",
        "else:\n",
        "    print(\"No correlations with p-value < 0.05 found.\")"
      ],
      "metadata": {
        "id": "5Fe68wdybVxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANOVA"
      ],
      "metadata": {
        "id": "JBq3piI6c9Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "for i in range(10):\n",
        "  nr =  't' + str(i + 1) # 't' +\n",
        "  wai_scores = df[nr].astype('category')\n",
        "  for feature in features:\n",
        "      groups = []\n",
        "      for category in wai_scores.cat.categories:\n",
        "          groups.append(df[feature][wai_scores == category])\n",
        "\n",
        "      # Perform ANOVA test\n",
        "      f_statistic, p_value = f_oneway(*groups)\n",
        "\n",
        "      if p_value < 0.05:\n",
        "          anova_results.append([f'WAI Question {nr}', feature, f\"{f_statistic:.2f}\", f\"{p_value:.4f}\"])\n",
        "\n",
        "# Print results in a table\n",
        "if anova_results:\n",
        "    headers = [\"WAI Question\", \"Feature\", \"F-Statistic\", \"p-value\"]\n",
        "    table = tabulate(anova_results, headers=headers, tablefmt=\"pretty\")\n",
        "\n",
        "    # Save the table as a vector PDF\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.axis('off')\n",
        "    plt.table(cellText=anova_results, colLabels=headers, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(headers))\n",
        "    plt.savefig('anova_table_p.pdf', format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"ANOVA Results:\")\n",
        "    print(table)\n",
        "else:\n",
        "    print(\"No significant ANOVA results (p-value < 0.05) found.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "blTih_UXbVxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## heatmap for correlation between features"
      ],
      "metadata": {
        "id": "fL_B8e_GbVxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the features\n",
        "features = ['nr_of_turns', 'turns_therapist', 'turns_speaker_patient', 'duration_therapist', 'duration_patient',\n",
        "            'participation_equality', 'turn_level_freedom', 'avgturnlength_therapist', 'avgturnlength_patient',\n",
        "            'wordcount_speaker_therapist', 'wordcount_speaker_patient', 'speechrate_speaker_therapist',\n",
        "            'speechrate_speaker_patient', 'speechduration_therapist', 'speechduration_patient',\n",
        "            'durationpercentage_therapist', 'durationpercentage_patient']\n",
        "\n",
        "# Create a correlation matrix\n",
        "correlation_matrix = df[features].corr()\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "68BKa2sQbVxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observer scores"
      ],
      "metadata": {
        "id": "nslyXpwksj5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('allfeat_incl_O_WAI2.csv', delimiter=',')\n",
        "\n",
        "new_column_names = {str(i): f'o{i}' for i in range(1, 13)}\n",
        "df.rename(columns=new_column_names, inplace=True)\n",
        "\n",
        "# List of column names\n",
        "columns_to_check = ['o1', 'o2', 'o3', 'o4', 'o5', 'o6', 'o7', 'o8', 'o9', 'o10','o11', 'o12']\n",
        "\n",
        "# Convert the specified columns to numeric, replacing invalid values with NaN\n",
        "df[columns_to_check] = df[columns_to_check].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Check for non-float values in the specified columns\n",
        "non_float_mask = ~df[columns_to_check].applymap(lambda x: isinstance(x, float))\n",
        "\n",
        "# Find rows where any of the specified columns have non-float values\n",
        "rows_with_non_floats = non_float_mask.any(axis=1)\n",
        "\n",
        "df = df.drop_duplicates(subset=['ppnr', 'o1', 'o2', 'o3', 'o4', 'o5', 'o6', 'o7', 'o8', 'o9', 'o10','o11', 'o12'])\n",
        "df = df.dropna(subset=columns_to_check, how='any')\n",
        "\n",
        "# If needed, reset the index of the filtered DataFrame\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "2ZGGSVm2smh5"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Assuming you have a DataFrame called 'df' with the desired columns\n",
        "bond_columns = ['o3', 'o5', 'o7','o9']\n",
        "goal_columns = ['o1', 'o4', 'o8','o11']\n",
        "task_columns = ['o2', 'o6', 'o10','o12']\n",
        "\n",
        "# Calculate the row-wise average for the specified columns\n",
        "bond_df = df[bond_columns].mean(axis=1)\n",
        "goal_df = df[goal_columns].mean(axis=1)\n",
        "task_df = df[task_columns].mean(axis=1)\n",
        "\n",
        "# Calculate the row-wise average for the specified columns\n",
        "bond_df = df[bond_columns].mean(axis=1)\n",
        "goal_df = df[goal_columns].mean(axis=1)\n",
        "task_df = df[task_columns].mean(axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "4EpWXgDysmiB"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df #68 rows"
      ],
      "metadata": {
        "id": "TutW2OG6LeFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF"
      ],
      "metadata": {
        "id": "E-xQ_C1dsmiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load data from CSV\n",
        "all_df = df\n",
        "\n",
        "# Convert numeric columns to float\n",
        "numeric_columns = ['ppnr', 'session']\n",
        "all_df[numeric_columns] = all_df[numeric_columns].astype(float)\n",
        "\n",
        "df = all_df\n",
        "\n",
        "# Select features and target variable\n",
        "features = df[[ 'mean_waittime', 'median_waittime', 'std_waittime','nr_of_turns', 'turns_therapist',\n",
        "       'turns_speaker_patient', 'duration_therapist', 'duration_patient',\n",
        "       'participation_equality', 'turn_level_freedom',\n",
        "       'avgturnlength_therapist', 'avgturnlength_patient',\n",
        "       'wordcount_speaker_therapist', 'wordcount_speaker_patient',\n",
        "       'speechrate_speaker_therapist', 'speechrate_speaker_patient',\n",
        "       'speechduration_therapist', 'speechduration_patient',\n",
        "       'durationpercentage_therapist', 'durationpercentage_patient']]\n",
        "\n",
        "\n",
        "for i in range(12):\n",
        "    nr = 'o' + str(i+1)\n",
        "\n",
        "    outcome = all_df[nr]\n",
        "    print(\"WAI question:\", nr)\n",
        "\n",
        "    # Handle missing values in features using an imputer\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "    # Train the model\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    t.fit(features_imputed, outcome)\n",
        "\n",
        "    # Calculate out-of-bag MSE for Outcome 1\n",
        "    oob_error = 1 - t.score(features_imputed, outcome)\n",
        "    print(\"Out-of-Bag MSE for Outcome 1:\", oob_error)\n",
        "\n",
        "    # Get feature importances\n",
        "    impOOB = t.feature_importances_\n",
        "\n",
        "    # Plot feature importances\n",
        "    plt.bar(range(len(impOOB)), impOOB)\n",
        "    plt.title('Unbiased Predictor Importance Estimates')\n",
        "    plt.xlabel('Predictor variable')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xticks(range(len(impOOB)), features.columns, rotation=90)\n",
        "    # plt.show()\n",
        "\n",
        "    # Check by predicting held back data\n",
        "    ncells = features.shape[0]\n",
        "    perc = int(0.8 * ncells)\n",
        "\n",
        "    nboots = 10\n",
        "    Acc = np.empty(nboots)\n",
        "    for b in range(nboots):\n",
        "        shuf = np.random.permutation(ncells)\n",
        "        incl = shuf[:perc]\n",
        "        holdback = shuf[perc:]\n",
        "\n",
        "        Mdl = t.fit(features.iloc[incl], outcome.iloc[incl])\n",
        "        label = Mdl.predict(features.iloc[holdback])\n",
        "        Acc[b] = mean_squared_error(outcome.iloc[holdback], label)\n",
        "\n",
        "    mean_mse = np.mean(Acc)\n",
        "    print(\"Mean MSE for Outcome 1:\", mean_mse)\n",
        "\n",
        "    # Feature selection\n",
        "    sorted_indices = np.argsort(impOOB)\n",
        "    bestfeat = sorted_indices[7:]\n",
        "    t = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "    MdlBF = t.fit(features.iloc[:, bestfeat], outcome)\n",
        "    oob_error_bf = 1 - MdlBF.score(features.iloc[:, bestfeat], outcome)\n",
        "    print(\"Out-of-Bag MSE (Feature Selection):\", oob_error_bf)\n",
        "\n",
        "    # Print the selected features\n",
        "    selected_features = features.columns[bestfeat]\n",
        "    print(\"Selected Features:\", selected_features)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ubss1g3gsmiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF for components"
      ],
      "metadata": {
        "id": "dCnJ40rVLEV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "df = all_df\n",
        "\n",
        "# Select features\n",
        "features = df[['nr_of_turns', 'turns_therapist',\n",
        "       'turns_speaker_patient', 'duration_therapist', 'duration_patient',\n",
        "       'participation_equality', 'turn_level_freedom',\n",
        "       'avgturnlength_therapist', 'avgturnlength_patient',\n",
        "       'wordcount_speaker_therapist', 'wordcount_speaker_patient',\n",
        "       'speechrate_speaker_therapist', 'speechrate_speaker_patient',\n",
        "       'speechduration_therapist', 'speechduration_patient',\n",
        "       'durationpercentage_therapist', 'durationpercentage_patient']]\n",
        "# Prepare outcome variables\n",
        "outcome_bond = bond_df\n",
        "outcome_goal = goal_df\n",
        "outcome_task = task_df\n",
        "\n",
        "# Handle missing values in features using an imputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "# Train the model for bond_df\n",
        "t_bond = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "t_bond.fit(features_imputed, outcome_bond)\n",
        "\n",
        "# Train the model for goal_df\n",
        "t_goal = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "t_goal.fit(features_imputed, outcome_goal)\n",
        "\n",
        "# Train the model for task_df\n",
        "t_task = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1)\n",
        "t_task.fit(features_imputed, outcome_task)\n",
        "\n",
        "# Calculate out-of-bag MSE for bond_df\n",
        "oob_error_bond = 1 - t_bond.score(features_imputed, outcome_bond)\n",
        "print(\"Out-of-Bag MSE for bond_df:\", oob_error_bond)\n",
        "\n",
        "# Calculate out-of-bag MSE for goal_df\n",
        "oob_error_goal = 1 - t_goal.score(features_imputed, outcome_goal)\n",
        "print(\"Out-of-Bag MSE for goal_df:\", oob_error_goal)\n",
        "\n",
        "# Calculate out-of-bag MSE for task_df\n",
        "oob_error_task = 1 - t_task.score(features_imputed, outcome_task)\n",
        "print(\"Out-of-Bag MSE for task_df:\", oob_error_task)\n",
        "\n",
        "# Feature importances for bond_df\n",
        "print(\"Feature importances for bond_df:\", t_bond.feature_importances_)\n",
        "\n",
        "# Feature importances for goal_df\n",
        "print(\"Feature importances for goal_df:\", t_goal.feature_importances_)\n",
        "\n",
        "# Feature importances for task_df\n",
        "print(\"Feature importances for task_df:\", t_task.feature_importances_)\n",
        "\n",
        "# Plots (if desired)\n",
        "# Plot feature importances for bond_df\n",
        "impOOB_bond = t_bond.feature_importances_\n",
        "plt.bar(range(len(impOOB_bond)), impOOB_bond)\n",
        "plt.title('Feature Importance for bond_df')\n",
        "plt.xlabel('Predictor variable')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(range(len(impOOB_bond)), features.columns, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances for goal_df\n",
        "impOOB_goal = t_goal.feature_importances_\n",
        "plt.bar(range(len(impOOB_goal)), impOOB_goal)\n",
        "plt.title('Feature Importance for goal_df')\n",
        "plt.xlabel('Predictor variable')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(range(len(impOOB_goal)), features.columns, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances for task_df\n",
        "impOOB_task = t_task.feature_importances_\n",
        "plt.bar(range(len(impOOB_task)), impOOB_task)\n",
        "plt.title('Feature Importance for task_df')\n",
        "plt.xlabel('Predictor variable')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(range(len(impOOB_task)), features.columns, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Cross-validation for bond_df\n",
        "predicted_bond = cross_val_predict(t_bond, features_imputed, outcome_bond, cv=10)\n",
        "mse_bond = mean_squared_error(outcome_bond, predicted_bond)\n",
        "print(\"Cross-validation MSE for bond_df:\", mse_bond)\n",
        "\n",
        "# Cross-validation for goal_df\n",
        "predicted_goal = cross_val_predict(t_goal, features_imputed, outcome_goal, cv=10)\n",
        "mse_goal = mean_squared_error(outcome_goal, predicted_goal)\n",
        "print(\"Cross-validation MSE for goal_df:\", mse_goal)\n",
        "\n",
        "# Cross-validation for task_df\n",
        "predicted_task = cross_val_predict(t_task, features_imputed, outcome_task, cv=10)\n",
        "mse_task = mean_squared_error(outcome_task, predicted_task)\n",
        "print(\"Cross-validation MSE for task_df:\", mse_task)\n",
        "\n",
        "# Mean MSE for bond_df\n",
        "mean_mse_bond = np.mean(mse_bond)\n",
        "print(\"Mean MSE for bond_df:\", mean_mse_bond)\n",
        "\n",
        "# Mean MSE for goal_df\n",
        "mean_mse_goal = np.mean(mse_goal)\n",
        "print(\"Mean MSE for goal_df:\", mean_mse_goal)\n",
        "\n",
        "# Mean MSE for task_df\n",
        "mean_mse_task = np.mean(mse_task)\n",
        "print(\"Mean MSE for task_df:\", mean_mse_task)\n"
      ],
      "metadata": {
        "id": "Vi_CUkSlLEV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pearson Correlation"
      ],
      "metadata": {
        "id": "r2HLVmYjsmiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr, linregress\n",
        "from tabulate import tabulate\n",
        "\n",
        "correlation_results = []\n",
        "\n",
        "# Calculate correlations and slopes\n",
        "for feature in features:\n",
        "    correlated_wai_questions = []\n",
        "\n",
        "    for wai_question in range(1, 13):\n",
        "        question_col = 'o' + str(wai_question)\n",
        "        val = feature\n",
        "\n",
        "        # Calculate Pearson correlation coefficient and p-value\n",
        "        correlation_coefficient, p_value = pearsonr(df[val].astype(float), df[question_col].astype(float))\n",
        "\n",
        "        if p_value < 0.05:\n",
        "            # Calculate the slope of linear regression\n",
        "            slope, _, _, _, _ = linregress(df[val].astype(float), df[question_col].astype(float))\n",
        "            correlated_wai_questions.append([f'WAI Question {wai_question}', f\"{correlation_coefficient:.2f} ({slope:.2f})\", f\"{p_value:.4f}\"])\n",
        "\n",
        "    if correlated_wai_questions:\n",
        "        correlation_results.append([feature, correlated_wai_questions])\n",
        "\n",
        "# Print results in a table\n",
        "if correlation_results:\n",
        "    headers = [\"Feature\", \"WAI (Factors)\",\"Correlation Coefficient (Slope)\", \"p-value\"]\n",
        "    table_data = []\n",
        "    for result in correlation_results:\n",
        "        feature_name = result[0]\n",
        "        for corr_data in result[1]:\n",
        "            table_data.append([feature_name] + corr_data)\n",
        "\n",
        "    table = tabulate(table_data, headers=headers, tablefmt=\"pretty\")\n",
        "    # Save the table as a vector PDF\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.axis('off')\n",
        "    plt.table(cellText=table_data, colLabels=headers, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(headers))\n",
        "    plt.savefig('correlation_table_t.pdf', format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Correlation Results for the observer scores Trento features:\")\n",
        "    print(table)\n",
        "else:\n",
        "    print(\"No correlations with p-value < 0.05 found.\")"
      ],
      "metadata": {
        "id": "aviYSrTxsmiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANOVA"
      ],
      "metadata": {
        "id": "khJaSPkrChE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "from tabulate import tabulate\n",
        "\n",
        "anova_results = []\n",
        "\n",
        "# Perform ANOVA test for each question and feature\n",
        "for i in range(12):\n",
        "    nr = 'o'+ str(i + 1)  # 't' +\n",
        "    wai_scores = df[nr].astype('category')\n",
        "    for feature in features:\n",
        "        groups = []\n",
        "        for category in wai_scores.cat.categories:\n",
        "            groups.append(df[feature][wai_scores == category])\n",
        "\n",
        "        # Perform ANOVA test\n",
        "        f_statistic, p_value = f_oneway(*groups)\n",
        "\n",
        "        if p_value < 0.05:\n",
        "            anova_results.append([f'WAI Question {nr}', feature, f\"{f_statistic:.2f}\", f\"{p_value:.4f}\"])\n",
        "\n",
        "# Print results in a table\n",
        "if anova_results:\n",
        "    headers = [\"WAI Question\", \"Feature\", \"F-Statistic\", \"p-value\"]\n",
        "    table = tabulate(anova_results, headers=headers, tablefmt=\"pretty\")\n",
        "\n",
        "    # Save the table as a vector PDF\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.axis('off')\n",
        "    plt.table(cellText=anova_results, colLabels=headers, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(headers))\n",
        "    plt.savefig('anova_table_p.pdf', format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"ANOVA Results:\")\n",
        "    print(table)\n",
        "else:\n",
        "    print(\"No significant ANOVA results (p-value < 0.05) found.\")\n"
      ],
      "metadata": {
        "id": "DAuvm_uv2ft9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## heatmap for correlation between features"
      ],
      "metadata": {
        "id": "EplIa49m2fuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the features\n",
        "features = ['nr_of_turns', 'turns_therapist', 'turns_speaker_patient', 'duration_therapist', 'duration_patient',\n",
        "            'participation_equality', 'turn_level_freedom', 'avgturnlength_therapist', 'avgturnlength_patient',\n",
        "            'wordcount_speaker_therapist', 'wordcount_speaker_patient', 'speechrate_speaker_therapist',\n",
        "            'speechrate_speaker_patient', 'speechduration_therapist', 'speechduration_patient',\n",
        "            'durationpercentage_therapist', 'durationpercentage_patient']\n",
        "\n",
        "# Create a correlation matrix\n",
        "correlation_matrix = df[features].corr()\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GBk3PDe12fuN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
