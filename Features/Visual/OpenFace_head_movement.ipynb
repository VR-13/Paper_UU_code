{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X38L6tanrnrB"
      },
      "source": [
        "# Facial Feature Detection with OpenFace\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXBkyjI_GqtQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "################# Need to revert back to CUDA 10.0 ##################\n",
        "# Thanks to http://aconcaguasci.blogspot.com/2019/12/setting-up-cuda-100-for-mxnet-on-google.html\n",
        "#Uninstall the current CUDA version\n",
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update\n",
        "\n",
        "#Download CUDA 10.0\n",
        "!wget  --no-clobber https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "#install CUDA kit dpkg\n",
        "!dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "!sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-10-0\n",
        "#Slove libcurand.so.10 error\n",
        "!wget --no-clobber http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "#-nc, --no-clobber: skip downloads that would download to existing files.\n",
        "!apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "!apt-get update\n",
        "####################################################################\n",
        "\n",
        "git_repo_url = 'https://github.com/TadasBaltrusaitis/OpenFace.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "# clone openface\n",
        "!git clone -q --depth 1 $git_repo_url\n",
        "\n",
        "# install new CMake becaue of CUDA10\n",
        "!wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "!tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "\n",
        "# Get newest GCC\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential\n",
        "!sudo apt-get install g++-8\n",
        "\n",
        "#added 5/15/2022. Thanks to @weskhoo\n",
        "!sudo apt-key del 7fa2af80\n",
        "!sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/3bf863cc.pub\n",
        "\n",
        "# install python dependencies\n",
        "!pip install -q youtube-dl\n",
        "\n",
        "# Finally, actually install OpenFace\n",
        "!cd OpenFace && bash ./download_models.sh && sudo bash ./install.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5L3Z5YVrZ2R"
      },
      "source": [
        "## Detect facial expressions on a test video\n",
        "\n",
        "We are going to detect facial features on the following Youtube video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIt-eyIDO6XG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "# Change the Youtube_ID with the link to your group's video.\n",
        "YOUTUBE_ID = 'XtA6FQz8BHQ'\n",
        "\n",
        "YouTubeVideo(YOUTUBE_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn08K-3bp-W9"
      },
      "source": [
        "Download the above youtube video, cut the first 10 seconds and do the face detection & feature extraction on that clip. This takes about a minute or two. Instead of `FaceLandmarkVidMulti` you may also use `FeatureExtraction` to extract features of a single face or `FaceLandmarkImg` to extract features on a face image. See full description of the arguments [here](https://github.com/TadasBaltrusaitis/OpenFace/wiki/Command-line-arguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00-3ZgVEHskf"
      },
      "outputs": [],
      "source": [
        "!rm -rf youtube.mp4\n",
        "# download the youtube with the given ID\n",
        "!youtube-dl -f 'bestvideo[ext=mp4]' --output \"youtube.%(ext)s\" https://www.youtube.com/watch?v=$YOUTUBE_ID\n",
        "# cut the first 5 seconds\n",
        "!ffmpeg -y -loglevel info -i youtube.mp4 -t 10 video.mp4\n",
        "# clear any previous outputs.\n",
        "!rm -rf processed\n",
        "# detect poses on the these 10 seconds.\n",
        "!./OpenFace/build/bin/FaceLandmarkVidMulti -f video.mp4 -out_dir processed\n",
        "# convert the result into MP4\n",
        "!ffmpeg -y -loglevel info -i processed/video.avi output.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vffPDVDacvg9"
      },
      "outputs": [],
      "source": [
        "# input my own video\n",
        "# cut the first X seconds\n",
        "!ffmpeg -y -loglevel info -i emotions.mov -t 10 video2.mov\n",
        "\n",
        "# detect poses on the these X seconds.\n",
        "!./bin/FaceLandmarkVid -f 'video2.mov' #-out_dir\n",
        "# convert the result into MP4\n",
        "!ffmpeg -y -loglevel info -i video2.mov output2.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDDkgCCSrFTv"
      },
      "source": [
        "Finally, visualize the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ3Ud9zLgOoQ"
      },
      "outputs": [],
      "source": [
        "def show_local_mp4_video(file_name, width=640, height=480):\n",
        "  import io\n",
        "  import base64\n",
        "  from IPython.display import HTML\n",
        "  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
        "  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
        "                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
        "                      </video>'''.format(width, height, video_encoded.decode('ascii')))\n",
        "\n",
        "show_local_mp4_video('/content/output2.mp4', width=960, height=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF-oLibrVqY3"
      },
      "source": [
        "# Postprocess faces"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the folder path containing your CSV files\n",
        "folder_path = ''\n",
        "\n",
        "# Function to threshold the data and save to a new CSV file\n",
        "def threshold_and_save(file_path, threshold=0.70):\n",
        "    # Check if the file is empty or does not exist\n",
        "    if os.path.getsize(file_path) == 0:\n",
        "        print(f\"Skipping empty file: {file_path}\")\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "    # Check if the DataFrame is empty\n",
        "    if df.empty:\n",
        "        print(f\"DataFrame is empty after reading, skipping file: {file_path}\")\n",
        "        return\n",
        "\n",
        "    # Remove empty spaces in column names\n",
        "    df.columns = [col.replace(\" \", \"\") for col in df.columns]\n",
        "    # Convert 'confidence' column to numeric (float) if it's not already\n",
        "    df['confidence'] = df['confidence'].astype(float)\n",
        "    # Threshold the data\n",
        "    df_clean = df[df.confidence >= threshold]\n",
        "\n",
        "    # Check if the DataFrame is empty after thresholding\n",
        "    if df_clean.empty:\n",
        "        print(f\"Empty DataFrame after thresholding, skipping file: {file_path}\")\n",
        "        return\n",
        "\n",
        "    # Save the clean DataFrame to a new CSV file\n",
        "    file_name = os.path.basename(file_path)\n",
        "    clean_file_path = os.path.join('', file_name)\n",
        "    df_clean.to_csv(clean_file_path, index=False)\n",
        "\n",
        "# Loop through files in the folder and process each one\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv') and not file_name.startswith('._'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        threshold_and_save(file_path)\n"
      ],
      "metadata": {
        "id": "cflRAggcbZ_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the folder containing the CSV files\n",
        "csv_folder = \"\"\n",
        "\n",
        "# Define the output CSV file\n",
        "output_csv_file = \"\"\n",
        "\n",
        "# Initialize the output data list with header\n",
        "output_data = [['Filename', 'neutral', 'anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise', 'contempt', 'pain', 'unknown']]\n",
        "\n",
        "# Initialize an empty DataFrame to store the combined data\n",
        "combined_data = pd.DataFrame()\n",
        "\n",
        "\n",
        "# Loop through all CSV files in the folder\n",
        "csv_files = [file for file in os.listdir(csv_folder) if file.endswith(\".csv\")]\n",
        "for csv_file in csv_files:\n",
        "    csv_path = os.path.join(csv_folder, csv_file)\n",
        "\n",
        "    # Read the first row from the CSV file with pandas\n",
        "    df = pd.read_csv(csv_path, nrows=1, encoding='ISO-8859-1')\n",
        "\n",
        "    # Append the filename as the first element in the first row\n",
        "    df.insert(0, 'Filename', csv_file)\n",
        "\n",
        "    # Append the data to the combined DataFrame\n",
        "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
        "\n",
        "# Write the combined data to the output CSV file\n",
        "combined_data.to_csv(output_csv_file, index=False)\n",
        "\n",
        "print(\"Data from all files has been combined and saved to\", output_csv_file)\n"
      ],
      "metadata": {
        "id": "AFya2E4vDifZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## determine the percenatage that a specific emotion occurs instead of counts"
      ],
      "metadata": {
        "id": "2GtJxzns4x-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "input_csv_file = \"\"  # Replace with the path to your CSV file\n",
        "output_csv_file = \"\"  # Replace with the desired output file path\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(input_csv_file, delimiter=';')\n",
        "\n",
        "# Calculate the total count for each row\n",
        "total_emotions = df[['neutral','anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise', 'contempt', 'pain', 'unknown']].sum(axis=1)\n",
        "\n",
        "# Calculate the percentage of each emotion compared to the total and replace the counts\n",
        "emotions = ['neutral','anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise', 'contempt', 'pain', 'unknown']\n",
        "for emotion in emotions:\n",
        "    df[emotion] = df[emotion] / total_emotions * 100\n",
        "\n",
        "# Save the modified DataFrame back to a new CSV file\n",
        "df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "print(\"Percentage data has been calculated and saved to\", output_csv_file)\n"
      ],
      "metadata": {
        "id": "-U6wX1KOGlW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## threshold based on likelihood of it being a face"
      ],
      "metadata": {
        "id": "bdl_N7-65Apt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybpgggJUC14U"
      },
      "outputs": [],
      "source": [
        "# Threshold data by 75%\n",
        "df_clean = df[df.confidence>=.75]\n",
        "# Plot all Action Unit time series.\n",
        "au_regex_pat = re.compile(r'^AU[0-9]+_r$')\n",
        "au_columns = df.columns[df.columns.str.contains(au_regex_pat)]\n",
        "print(\"List of AU columns:\", au_columns)\n",
        "f,axes = plt.subplots(6, 3, figsize=(10,12), sharex=True, sharey=True)\n",
        "axes = axes.flatten()\n",
        "for au_ix, au_col in enumerate(au_columns):\n",
        "    sns.lineplot(x='frame', y=au_col, hue='face_id', data=df_clean, ax=axes[au_ix])\n",
        "    axes[au_ix].set(title=au_col, ylabel='Intensity')\n",
        "    axes[au_ix].legend(loc=5)\n",
        "plt.suptitle(\"AU intensity predictions by time for each face\", y=1.02)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVHPblTMI-Sn"
      },
      "source": [
        "We could also compare how synchronized each individuals are to one another during the interaction by using a simple Pearson correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5M8BUfHWFwx"
      },
      "outputs": [],
      "source": [
        "# Let's compare how much AU12 (smiling) activity occurs at similar times across people.\n",
        "df_clean.pivot(index='frame', columns='face_id', values='AU12_r').corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* AU1: Inner Brow Raiser - Associated with surprise or fear.\n",
        "* AU2: Outer Brow Raiser - Associated with surprise or fear.\n",
        "* AU4: Brow Lowerer - Associated with anger or sadness.\n",
        "* AU6: Cheek Raiser - Associated with happiness or fear.\n",
        "* AU7: Lid Tightener - Associated with fear or anger.\n",
        "* AU9: Nose Wrinkler - Associated with disgust.\n",
        "* AU10: Upper Lip Raiser - Associated with disgust or sadness.\n",
        "* AU12: Lip Corner Puller (Smile) - Associated with happiness.\n",
        "* AU14: Dimpler - Associated with happiness.\n",
        "* AU15: Lip Corner Depressor - Associated with sadness.\n",
        "* AU17: Chin Raiser - Associated with sadness or determination.\n",
        "* AU20: Lip Stretcher - Associated with happiness or surprise.\n",
        "* AU23: Lip Tightener - Associated with fear or determination.\n",
        "* AU24: Lip Pressor - Associated with sadness or anger.\n",
        "* AU25: Lips Part - Associated with surprise or fear.\n",
        "* AU26: Jaw Drop - Associated with surprise."
      ],
      "metadata": {
        "id": "WwlJSMXAQ2D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for each individual feature\n",
        "# Select the relevant columns for AUs\n",
        "au_columns = ['AU01_r', 'AU02_r', 'AU04_r', 'AU06_r', 'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r',\n",
        "              'AU14_r', 'AU15_r', 'AU17_r', 'AU20_r', 'AU23_r']\n",
        "\n",
        "# Loop through each AU and calculate its correlation matrix\n",
        "for au in au_columns:\n",
        "    correlation_matrix = df_clean.pivot(index='frame', columns='face_id', values=au).corr()\n",
        "    print(f\"Correlation matrix for {au}:\")\n",
        "    print(correlation_matrix)\n",
        "    print('\\n')\n"
      ],
      "metadata": {
        "id": "wXk03SBPQr3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for all features at the same time\n",
        "correlation_matrix = df_clean.pivot(index='frame', columns='face_id', values=au_columns).corr()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "x-2bFfxiQu4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuIELqKU84fV"
      },
      "source": [
        "# Gaze directions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfVyvCvH84sR"
      },
      "outputs": [],
      "source": [
        "f,axes = plt.subplots(2,len(df_clean.face_id.unique()), figsize=(10,5))\n",
        "for faces_ix, face_id in enumerate(df_clean.face_id.unique()[::-1]):\n",
        "  df_clean.query(f'face_id=={face_id}').plot.scatter(x='gaze_angle_x', y='gaze_angle_y', ax=axes[0][faces_ix])\n",
        "  axes[0][faces_ix].scatter(0,0, marker='x', color = 'k') # draw origin.\n",
        "  axes[0][faces_ix].set(xlim=[-2,2], ylim=[-2,2], title=f'Gaze movement of face_id=={face_id}')\n",
        "  df_clean.query(f'face_id=={face_id}')[['gaze_angle_x', 'gaze_angle_y']].plot(ax=axes[1][faces_ix])\n",
        "  axes[1][faces_ix].set(ylim=[-1.5,1.5], xlabel='Frame Number', ylabel=\"Radians\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8s3gDRe96kc"
      },
      "source": [
        "#Track Nodding Feature"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "kituYqCa6PkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# Define the folder containing the CSV files\n",
        "csv_folder = \"/content/openface\"\n",
        "\n",
        "# Distance function\n",
        "def distance(x, y):\n",
        "    import math\n",
        "    return math.sqrt((x[0]-y[0])**2 + (x[1]-y[1])**2)\n",
        "\n",
        "# Parameters for lucas kanade optical flow\n",
        "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "\n",
        "# Function to get coordinates\n",
        "def get_coords(p1):\n",
        "    try:\n",
        "        return int(p1[0][0][0]), int(p1[0][0][1])\n",
        "    except:\n",
        "        return int(p1[0][0]), int(p1[0][1])\n",
        "\n",
        "# Define font and text color\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "# Define movement thresholds\n",
        "max_head_movement = 20\n",
        "movement_threshold = 50\n",
        "gesture_threshold = 175\n",
        "\n",
        "# Find the face in the image\n",
        "def find_face(frame_gray):\n",
        "    faces = face_cascade.detectMultiScale(frame_gray, 1.3, 5)\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        face_center = x + w/2, y + h/3\n",
        "        p0 = np.array([[face_center]], np.float32)\n",
        "        return frame, p0, face_center\n",
        "    return frame, None, None\n",
        "\n",
        "# Initialize gesture variables\n",
        "gesture = False\n",
        "x_movement = 0\n",
        "y_movement = 0\n",
        "gesture_show = 60  # number of frames a gesture is shown\n",
        "\n",
        "# Define the dimensions of your image frames (replace these with actual values)\n",
        "width = 640\n",
        "height = 480\n",
        "channels = 3  # Assuming BGR images, set to 1 for grayscale images\n",
        "\n",
        "# Loop through all CSV files in the folder\n",
        "total_nods = 0\n",
        "total_shakes = 0\n",
        "csv_files = [file for file in os.listdir(csv_folder) if file.endswith(\".csv\")]\n",
        "for csv_file in csv_files:\n",
        "    csv_path = os.path.join(csv_folder, csv_file)\n",
        "\n",
        "    # Open the CSV file and read the data\n",
        "    with open(csv_path, \"r\") as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        data = list(csv_reader)\n",
        "\n",
        "    frame_num = 0\n",
        "    p0 = None\n",
        "    face_found = False\n",
        "\n",
        "    # Flag to skip the first row (header row)\n",
        "    first_row_skipped = False\n",
        "\n",
        "    for row in data:\n",
        "        # Skip the first row (header row) if it hasn't been skipped yet\n",
        "        if not first_row_skipped:\n",
        "            first_row_skipped = True\n",
        "            continue\n",
        "\n",
        "        frame_num += 1\n",
        "\n",
        "        # Assuming each row contains pixel values separated by commas\n",
        "        pixel_values = [int(float(pixel)) for pixel in row]  # Convert the row data to integers\n",
        "\n",
        "                # Check if the number of elements in pixel_values matches the expected size for the frame\n",
        "        expected_size = width * height * channels\n",
        "        if len(pixel_values) != expected_size:\n",
        "            print(f\"Error: Invalid number of pixels in row {frame_num}. Expected {expected_size} pixels, but got {len(pixel_values)} pixels.\")\n",
        "            continue\n",
        "\n",
        "        # Reshape the pixel values into an image format (e.g., width x height x channels)\n",
        "        frame = np.array(pixel_values, dtype=np.uint8).reshape(width, height, channels)\n",
        "\n",
        "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert the frame to grayscale\n",
        "\n",
        "        if not face_found:\n",
        "            frame, p0, face_center = find_face(frame_gray)\n",
        "            if p0 is not None:\n",
        "                face_found = True\n",
        "\n",
        "        if not face_found:\n",
        "            continue\n",
        "\n",
        "        if p0 is not None:\n",
        "            ret, frame = cap.read()\n",
        "            p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
        "            cv2.circle(frame, get_coords(p1), 4, (0, 0, 255), -1)\n",
        "            cv2.circle(frame, get_coords(p0), 4, (255, 0, 0))\n",
        "\n",
        "            # Get the xy coordinates for points p0 and p1\n",
        "            a, b = get_coords(p0), get_coords(p1)\n",
        "            x_movement += abs(a[0] - b[0])\n",
        "            y_movement += abs(a[1] - b[1])\n",
        "\n",
        "            text = 'x_movement: ' + str(x_movement)\n",
        "            if not gesture:\n",
        "                cv2.putText(frame, text, (50, 50), font, 0.8, (0, 0, 255), 2)\n",
        "            text = 'y_movement: ' + str(y_movement)\n",
        "            if not gesture:\n",
        "                cv2.putText(frame, text, (50, 100), font, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "            if x_movement > gesture_threshold:\n",
        "                gesture = 'No'\n",
        "            if y_movement > gesture_threshold:\n",
        "                gesture = 'Yes'\n",
        "            if gesture and gesture_show > 0:\n",
        "                cv2.putText(frame, 'Gesture Detected: ' + gesture, (50, 50), font, 1.2, (0, 0, 255), 3)\n",
        "                gesture_show -= 1\n",
        "            if gesture_show == 0:\n",
        "                if gesture == 'No':\n",
        "                    total_shakes += 1\n",
        "                elif gesture == 'Yes':\n",
        "                    total_nods += 1\n",
        "\n",
        "                gesture = False\n",
        "                x_movement = 0\n",
        "                y_movement = 0\n",
        "                gesture_show = 60  # number of frames a gesture is shown\n",
        "\n",
        "            p0 = p1\n",
        "\n",
        "            cv2_imshow(frame)\n",
        "            out.write(frame)\n",
        "            cv2.waitKey(1)\n",
        "\n",
        "# Save the total nods and shakes to a new CSV file\n",
        "output_csv_path = \"nods_shakes_count.csv\"\n",
        "with open(output_csv_path, 'w', newline='') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow([\"Nods\", \"Shakes\"])\n",
        "    csv_writer.writerow([total_nods, total_shakes])\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ],
      "metadata": {
        "id": "RJBeVki4RvBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate head nods/shakes for the openface files"
      ],
      "metadata": {
        "id": "lIq0kqUUgVpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculates total head movments per file and saves it in one output csv file - more an overview and not necessarily useful for analysis"
      ],
      "metadata": {
        "id": "3x6y5fLq56HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def calculate_head_gestures(data_file, window_size=30, threshold=0.75):\n",
        "    # Load the OpenFace output file as a DataFrame\n",
        "    df = pd.read_csv(data_file, delimiter=',', encoding='latin-1')\n",
        "\n",
        "    # Calculate the total distance traveled along pitch and yaw dimensions over a rolling window\n",
        "    df['pitch_distance'] = np.sqrt(df['pose_Rx'].diff().pow(2))\n",
        "    df['yaw_distance'] = np.sqrt(df['pose_Rz'].diff().pow(2))\n",
        "\n",
        "    # Calculate the rolling sum of the distances over the window\n",
        "    df['pitch_rolling_sum'] = df['pitch_distance'].rolling(window=30, min_periods=1).sum()\n",
        "    df['yaw_rolling_sum'] = df['yaw_distance'].rolling(window=30, min_periods=1).sum()\n",
        "\n",
        "    # Calculate the top quartile of distance traveled for pitch and yaw dimensions\n",
        "    pitch_threshold = df['pitch_rolling_sum'].quantile(0.75)\n",
        "    yaw_threshold = df['yaw_rolling_sum'].quantile(0.75)\n",
        "\n",
        "    # Detect head nods and shakes based on the top quartile of distance traveled\n",
        "    head_nods = df[df['pitch_rolling_sum'] >= pitch_threshold]\n",
        "    head_shakes = df[df['yaw_rolling_sum'] >= yaw_threshold]\n",
        "\n",
        "    # Count the total number of head nods and shakes per speaker (face_id)\n",
        "    head_nods_count = head_nods.groupby('face_id').size()\n",
        "    head_shakes_count = head_shakes.groupby('face_id').size()\n",
        "\n",
        "    return head_nods_count, head_shakes_count\n",
        "\n",
        "def count_head_gestures_per_speaker(data_file, speaker_id, speaker_talk_time, framerate=44100):\n",
        "    head_nods, head_shakes = calculate_head_gestures(data_file, framerate=framerate)\n",
        "\n",
        "    # Filter head gestures for the specified speaker and during their talking time\n",
        "    speaker_head_nods = head_nods[(head_nods['face_id'] == int(speaker_id)) & (head_nods['timestamp'] >= speaker_talk_time[0]) & (head_nods['timestamp'] <= speaker_talk_time[1])]\n",
        "    speaker_head_shakes = head_shakes[(head_shakes['face_id'] == int(speaker_id)) & (head_shakes['timestamp'] >= speaker_talk_time[0]) & (head_shakes['timestamp'] <= speaker_talk_time[1])]\n",
        "\n",
        "    # Count the number of head nods and shakes during the speaker's talking time\n",
        "    num_head_nods = len(speaker_head_nods)\n",
        "    num_head_shakes = len(speaker_head_shakes)\n",
        "\n",
        "    return num_head_nods, num_head_shakes\n",
        "\n",
        "def extract_speaker_talk_times(transcription_file):\n",
        "    # Load the transcription file as a DataFrame\n",
        "    df_transcription = pd.read_csv(transcription_file, delimiter=',')\n",
        "\n",
        "    # Replace 'SPEAKER_00' with 0 and 'SPEAKER_01' with 1 in the 'speaker' column\n",
        "    df_transcription['speaker'] = df_transcription['speaker'].replace({'SPEAKER_00': 0, 'SPEAKER_01': 1})\n",
        "\n",
        "    # Determine the unique speaker IDs\n",
        "    speaker_ids = df_transcription['speaker'].unique()\n",
        "\n",
        "    # Create a dictionary to store the talking times for each speaker\n",
        "    speaker_talk_times = {speaker_id: [] for speaker_id in speaker_ids}\n",
        "\n",
        "    # Iterate through the DataFrame to extract the talking times\n",
        "    for index, row in df_transcription.iterrows():\n",
        "        speaker_id = row['speaker']\n",
        "        start_time = row['start']\n",
        "        end_time = row['end']\n",
        "\n",
        "        # Append the talking time as a tuple (start_time, end_time) to the speaker's entry\n",
        "        if not pd.isnull(speaker_id):\n",
        "            speaker_talk_times[speaker_id].append((start_time, end_time))\n",
        "\n",
        "    return speaker_talk_times\n",
        "\n",
        "# Function to process a single file and save the results\n",
        "def process_file(file_path):\n",
        "    head_nods_count, head_shakes_count = calculate_head_gestures(file_path)\n",
        "\n",
        "    # Get the file name without extension\n",
        "    file_name = os.path.basename(file_path).split('.')[0]\n",
        "\n",
        "    # Create a DataFrame with the file name as the first column\n",
        "    df_results = pd.DataFrame({\n",
        "        'File Name': [file_name],\n",
        "        'Total Head Nods': [head_nods_count],\n",
        "        'Total Head Shakes': [head_shakes_count]\n",
        "    })\n",
        "\n",
        "    return df_results\n",
        "\n",
        "# Example usage to loop through a folder and process each file\n",
        "folder_path = '/Volumes/My Passport/openface/threshold/csvfiles/'\n",
        "file_list = os.listdir(folder_path)\n",
        "\n",
        "df_final_results = pd.DataFrame()\n",
        "\n",
        "for file_name in file_list:\n",
        "    if file_name.endswith(\".csv\"):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        df_results = process_file(file_path)\n",
        "        df_final_results = pd.concat([df_final_results, df_results])\n",
        "\n",
        "# Save the final results to a new CSV file\n",
        "output_file_path = '/Volumes/My Passport/openface/threshold/output_file.csv'\n",
        "df_final_results.to_csv(output_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "zWUH8jJOR3Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save all head movements per file - needed for combination with speaker id"
      ],
      "metadata": {
        "id": "HEQ75Bb-51XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_head_gestures(data_file, window_size=30, threshold=0.75):\n",
        "    # Load the OpenFace output file as a DataFrame\n",
        "    df = pd.read_csv(data_file, delimiter=',', encoding='latin-1')\n",
        "\n",
        "    # Calculate the total distance traveled along pitch and yaw dimensions over a rolling window\n",
        "    df['pitch_distance'] = np.sqrt(df['pose_Rx'].diff().pow(2))\n",
        "    df['yaw_distance'] = np.sqrt(df['pose_Rz'].diff().pow(2))\n",
        "\n",
        "    # Calculate the rolling sum of the distances over the window\n",
        "    df['pitch_rolling_sum'] = df['pitch_distance'].rolling(window=30, min_periods=1).sum()\n",
        "    df['yaw_rolling_sum'] = df['yaw_distance'].rolling(window=30, min_periods=1).sum()\n",
        "\n",
        "    # Calculate the top quartile of distance traveled for pitch and yaw dimensions\n",
        "    pitch_threshold = df['pitch_rolling_sum'].quantile(0.75)\n",
        "    yaw_threshold = df['yaw_rolling_sum'].quantile(0.75)\n",
        "\n",
        "    # Detect head nods and shakes based on the top quartile of distance traveled\n",
        "    head_nods = df[df['pitch_rolling_sum'] >= pitch_threshold]\n",
        "    head_shakes = df[df['yaw_rolling_sum'] >= yaw_threshold]\n",
        "\n",
        "    # Create a list to store head gestures\n",
        "    head_gestures = []\n",
        "\n",
        "    # Find the consecutive frames for head nods\n",
        "    head_nod_frames = head_nods.index.to_series().diff() != 1\n",
        "    head_nod_segments = np.split(head_nods, head_nod_frames[head_nod_frames].index)\n",
        "\n",
        "    # Add head nods to the head_gestures list\n",
        "    for segment in head_nod_segments:\n",
        "        if len(segment) > 0:\n",
        "            start_time = segment['timestamp'].iloc[0]\n",
        "            end_time = segment['timestamp'].iloc[-1]\n",
        "            face_id = segment['face_id'].iloc[0]\n",
        "            head_gestures.append({'start_time': start_time, 'end_time': end_time, 'movement': 'nod', 'face_id': face_id})\n",
        "\n",
        "    # Find the consecutive frames for head shakes\n",
        "    head_shake_frames = head_shakes.index.to_series().diff() != 1\n",
        "    head_shake_segments = np.split(head_shakes, head_shake_frames[head_shake_frames].index)\n",
        "\n",
        "    # Add head shakes to the head_gestures list\n",
        "    for segment in head_shake_segments:\n",
        "        if len(segment) > 0:\n",
        "            start_time = segment['timestamp'].iloc[0]\n",
        "            end_time = segment['timestamp'].iloc[-1]\n",
        "            face_id = segment['face_id'].iloc[0]\n",
        "            head_gestures.append({'start_time': start_time, 'end_time': end_time, 'movement': 'shake', 'face_id': face_id})\n",
        "\n",
        "    # Create a DataFrame from the head_gestures list\n",
        "    head_gestures_df = pd.DataFrame(head_gestures)\n",
        "\n",
        "    return head_gestures_df\n",
        "\n",
        "def process_file(file_path):\n",
        "    head_gestures = calculate_head_gestures(file_path)\n",
        "\n",
        "    # Get the file name without extension\n",
        "    file_name = os.path.basename(file_path).split('.')[0]\n",
        "\n",
        "    # Save the head gestures to a new CSV file\n",
        "    output_file_path = f'/Volumes/My Passport/openface/threshold/movements/{file_name}_head_movements.csv'\n",
        "    head_gestures.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Example usage to loop through a folder and process each file\n",
        "folder_path = '/Volumes/My Passport/openface/threshold/csvfiles/'\n",
        "file_list = os.listdir(folder_path)\n",
        "\n",
        "for file_name in file_list:\n",
        "    if file_name.endswith(\".csv\") and not file_name.startswith('._'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        process_file(file_path)\n"
      ],
      "metadata": {
        "id": "0q-DTxUH17vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plot the head movements"
      ],
      "metadata": {
        "id": "OvFjRrVUWs84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot the head movements\n",
        "def plot_head_movements(file_path):\n",
        "    # Load the head gestures DataFrame from the output file\n",
        "    head_gestures = pd.read_csv(file_path)\n",
        "\n",
        "    # Set up the plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.title('Head Movements')\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.yticks([])\n",
        "\n",
        "    # Calculate the timestamp corresponding to 5 minutes\n",
        "    five_minutes_timestamp = 5 * 60\n",
        "\n",
        "    # Plot head nods as blue dots within the first 5 minutes\n",
        "    head_nods = head_gestures[(head_gestures['movement'] == 'nod') & (head_gestures['start_time'] <= five_minutes_timestamp)]\n",
        "    plt.scatter(head_nods['start_time'], [0] * len(head_nods), color='blue', label='Head Nods', marker='o', s=10)\n",
        "\n",
        "    # Plot head shakes as red dots within the first 5 minutes\n",
        "    head_shakes = head_gestures[(head_gestures['movement'] == 'shake') & (head_gestures['start_time'] <= five_minutes_timestamp)]\n",
        "    plt.scatter(head_shakes['start_time'], [0] * len(head_shakes), color='red', label='Head Shakes', marker='o', s=10)\n",
        "\n",
        "    # Set x-axis limits to show only the first 5 minutes\n",
        "    plt.xlim(0, five_minutes_timestamp)\n",
        "\n",
        "    # Display the plot with a legend\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage to plot the head movements based on the first output file in the folder\n",
        "output_folder_path = '/Volumes/My Passport/openface/threshold/movements/'\n",
        "output_file_list = os.listdir(output_folder_path)\n",
        "\n",
        "# Assuming the first output file in the list is the file of interest\n",
        "if output_file_list:\n",
        "    output_file_path = os.path.join(output_folder_path, output_file_list[1])\n",
        "    plot_head_movements(output_file_path)\n"
      ],
      "metadata": {
        "id": "Fx9jSAxs0dMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Match head movement with listening/speaking times - transcription files"
      ],
      "metadata": {
        "id": "zEO3MgLpgcs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load transcription file and head movement file\n",
        "transcription_file_path = '/Volumes/My Passport/whisper_take2/output/1025/segments1025 sessie 1.wav.csv'\n",
        "head_movement_file_path = '/Volumes/My Passport/openface/threshold/movements/1025 sessie 1_head_movements.csv'\n",
        "\n",
        "transcription_df = pd.read_csv(transcription_file_path)\n",
        "head_movement_df = pd.read_csv(head_movement_file_path)\n",
        "\n",
        "# Function to find the speaker for each head movement\n",
        "def find_speaker_for_movement(movement_time, transcription_df):\n",
        "    speakers = []\n",
        "    for _, row in transcription_df.iterrows():\n",
        "        if row['start'] <= movement_time <= row['end']:\n",
        "            speakers.append(row['speaker'])\n",
        "    return speakers\n",
        "\n",
        "# Find the corresponding speaker for each head movement\n",
        "head_movement_df['speakers'] = head_movement_df['start_time'].apply(lambda x: find_speaker_for_movement(x, transcription_df))\n",
        "\n",
        "# Function to check if speaker 1 or 0 is in the list of speakers for each head movement\n",
        "def check_speaker(speakers):\n",
        "    if 1 in speakers:\n",
        "        return 1\n",
        "    elif 0 in speakers:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Count head movements for speaker 1, speaker 0, and outside any speaking time\n",
        "count_within_speaker1 = len(head_movement_df[head_movement_df['speakers'].apply(lambda x: check_speaker(x)) == 1])\n",
        "count_within_speaker0 = len(head_movement_df[head_movement_df['speakers'].apply(lambda x: check_speaker(x)) == 0])\n",
        "count_outside_speaking_time = len(head_movement_df[head_movement_df['speakers'].apply(lambda x: check_speaker(x)) == -1])\n",
        "\n",
        "\n",
        "# Print the results\n",
        "print(f\"Head movements within speaking time of Speaker 1: {count_within_speaker1}\")\n",
        "print(f\"Head movements within speaking time of Speaker 0: {count_within_speaker0}\")\n",
        "print(f\"Head movements outside any speaking time: {count_outside_speaking_time}\")\n"
      ],
      "metadata": {
        "id": "NHR9DBFAE-D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have 'transcription_df' and 'head_movement_df' loaded correctly\n",
        "\n",
        "# Function to check if a given head movement overlaps with speaker speaking time and return the active speaker\n",
        "def get_active_speaker(row):\n",
        "    speaker_00_overlap = any((transcription_df['speaker'] == 'SPEAKER_00') & (row['start_time'] <= transcription_df['end']) & (row['end_time'] >= transcription_df['start']))\n",
        "    speaker_01_overlap = any((transcription_df['speaker'] == 'SPEAKER_01') & (row['start_time'] <= transcription_df['end']) & (row['end_time'] >= transcription_df['start']))\n",
        "\n",
        "    if speaker_00_overlap and not speaker_01_overlap:\n",
        "        return 'SPEAKER_00'\n",
        "    elif speaker_01_overlap and not speaker_00_overlap:\n",
        "        return 'SPEAKER_01'\n",
        "    else:\n",
        "        return 'BOTH_OR_NONE'\n",
        "\n",
        "# Add a new column to 'head_movement_df' to indicate overlap with speakers and the active speaker\n",
        "head_movement_df['overlap_with_speaker'] = head_movement_df.apply(get_active_speaker, axis=1)\n",
        "\n",
        "# Set up the plot for speaker speaking times\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title('Speaker Speaking Times')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Speaker')\n",
        "plt.yticks([0, 1], ['Speaker 0', 'Speaker 1'])\n",
        "\n",
        "# Plot the speaking times for each speaker as bars\n",
        "for _, row in transcription_df.iterrows():\n",
        "    plt.barh(y=row['speaker'], width=row['end'] - row['start'], left=row['start'], height=0.5, color='blue' if row['speaker'] == 'SPEAKER_00' else 'red')\n",
        "\n",
        "# Plot green dots for head movements during speaker speaking times\n",
        "speaker_head_movements = head_movement_df[head_movement_df['overlap_with_speaker'] != 'BOTH_OR_NONE']\n",
        "plt.scatter(speaker_head_movements['start_time'], [0.5] * len(speaker_head_movements), color='green', marker='o', s=50)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GpuJzQXIHAhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check which speaker during a head movement -listening/speaking"
      ],
      "metadata": {
        "id": "iRA5DDuEWaSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Function to check if a given head movement overlaps with speaker speaking time and return the active speaker\n",
        "def get_active_speaker(row, transcription_df):\n",
        "    speaker_00_overlap = any((transcription_df['speaker'] == 'SPEAKER_00') & (row['start_time'] <= transcription_df['end']) & (row['end_time'] >= transcription_df['start']))\n",
        "    speaker_01_overlap = any((transcription_df['speaker'] == 'SPEAKER_01') & (row['start_time'] <= transcription_df['end']) & (row['end_time'] >= transcription_df['start']))\n",
        "\n",
        "    if speaker_00_overlap and not speaker_01_overlap:\n",
        "        return '0'\n",
        "    elif speaker_01_overlap and not speaker_00_overlap:\n",
        "        return '1'\n",
        "    else:\n",
        "        return '2'\n",
        "\n",
        "# Folder paths for head movements and transcriptions\n",
        "head_movement_folder = '/Volumes/My Passport/openface/threshold/movements/'\n",
        "transcription_folder = '/Volumes/My Passport/output 3/transcripts_firstbatch/'\n",
        "\n",
        "# Loop through head movement CSV files\n",
        "for head_movement_file in os.listdir(head_movement_folder):\n",
        "    if head_movement_file.endswith('.csv') and not head_movement_file.startswith('._'):\n",
        "        # Extract the file name (without extension) to find the corresponding transcription file\n",
        "        file_name = os.path.splitext(head_movement_file)[0]\n",
        "        # Remove '_head_movements' from the head movement file name and add '.wav' to match the transcription file name\n",
        "        file_name_without_movement = file_name.replace('_head_movements', '') + '.wav.csv'\n",
        "        transcription_file = os.path.join(transcription_folder, 'segments' + file_name_without_movement)\n",
        "\n",
        "        if not os.path.isfile(transcription_file):\n",
        "            # If the corresponding transcription file does not exist, skip this head movement file and print the filename\n",
        "            print(f\"Transcription file not found for: {head_movement_file}\")\n",
        "            continue\n",
        "\n",
        "        # Read head movement and transcription CSV files into pandas dataframes\n",
        "        head_movement_df = pd.read_csv(os.path.join(head_movement_folder, head_movement_file))\n",
        "        try:\n",
        "          transcription_df = pd.read_csv(transcription_file)\n",
        "        except:\n",
        "          print('error wrong datatype',transcription_file)\n",
        "          continue\n",
        "\n",
        "        # Add the 'overlap_with_speaker' column to the head movement dataframe\n",
        "        head_movement_df['speaker'] = head_movement_df.apply(get_active_speaker, axis=1, transcription_df=transcription_df)\n",
        "\n",
        "        # Write the updated head movement dataframe back to the CSV file\n",
        "        head_movement_df.to_csv(os.path.join(head_movement_folder, head_movement_file), index=False)"
      ],
      "metadata": {
        "id": "DVLbDqHQLLES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## summarize the counts per file per speaker and movement"
      ],
      "metadata": {
        "id": "3PNqHYIYnu4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Folder path for CSV files\n",
        "folder_path = '/Volumes/My Passport/openface/threshold/movements/'\n",
        "\n",
        "# Initialize an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# Loop through CSV files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        # Read the CSV file into a pandas DataFrame\n",
        "        df = pd.read_csv(os.path.join(folder_path, file_name), encoding='latin-1')\n",
        "\n",
        "        # Check if 'face_id' column exists in the DataFrame\n",
        "        if 'face_id' not in df.columns or 'speaker' not in df.columns:\n",
        "            continue\n",
        "\n",
        "        # Create a dictionary to store counts for each face_id (0 and 1 if available)\n",
        "        counts = {'filename': file_name}\n",
        "\n",
        "        for face_id in [0, 1]:\n",
        "            face_df = df[df['face_id'] == face_id]  # Filter by face_id\n",
        "            if len(face_df) > 0:  # Check if data exists for the face_id\n",
        "                face_speaker_0_nod_count = face_df[(face_df['movement'] == 'nod') & (face_df['speaker'] == 0)].shape[0]\n",
        "                face_speaker_1_nod_count = face_df[(face_df['movement'] == 'nod') & (face_df['speaker'] == 1)].shape[0]\n",
        "                counts[f'nod_count_face_id_{face_id}_speaker_0'] = face_speaker_0_nod_count\n",
        "                counts[f'nod_count_face_id_{face_id}_speaker_1'] = face_speaker_1_nod_count\n",
        "\n",
        "                face_speaker_0_shake_count = face_df[(face_df['movement'] == 'shake') & (face_df['speaker'] == 0)].shape[0]\n",
        "                face_speaker_1_shake_count = face_df[(face_df['movement'] == 'shake') & (face_df['speaker'] == 1)].shape[0]\n",
        "                counts[f'shake_count_face_id_{face_id}_speaker_0'] = face_speaker_0_shake_count\n",
        "                counts[f'shake_count_face_id_{face_id}_speaker_1'] = face_speaker_1_shake_count\n",
        "\n",
        "        # Append the counts to the results list\n",
        "        results.append(counts)\n",
        "\n",
        "# Create a new DataFrame using the results list\n",
        "counts_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the new DataFrame\n",
        "print(counts_df)\n",
        "\n",
        "# Save the DataFrame to a new CSV file with face_id column included\n",
        "output_file_path = 'output_counts_file.csv'\n",
        "counts_df.to_csv(output_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "cY66irHJU4bZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
